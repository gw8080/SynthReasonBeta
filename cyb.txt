In the philosophy of language, metaphysics, and metasemantics, meaning "is a relationship between two sorts of things: signs and the kinds of things they intend, express, or signify".[1]

The types of meanings vary according to the types of the thing that is being represented. Namely:

There are the things in the world, which might have meaning;
There are things in the world that are also signs of other things in the world, and so, are always meaningful (i.e., natural signs of the physical world and ideas within the mind);
There are things that are necessarily meaningful such as words and nonverbal symbols.
The major contemporary positions of meaning come under the following partial definitions of meaning:

Psychological theories, involving notions of thought, intention, or understanding;
Logical theories, involving notions such as intension, cognitive content, or sense, along with extension, reference, or denotation;
Message, content, information, or communication;
Truth conditions;
Usage, and the instructions for usage; and
Measurement, computation, or operation.

The evaluation of meaning according to each one of the five major substantive theories of meaning and truth is presented below. The question of what is a proper basis for deciding how words, symbols, ideas and beliefs may properly be considered to truthfully denote meaning, whether by a single person or an entire society, is dealt with by the five most prevalent substantive theories listed below. Each theory of meaning as evaluated by these respective theories of truth are each further researched by the individual scholars supporting each one of the respective theories of truth and meaning.[2][3][4]

Both hybrid theories of meaning and alternative theories of meaning and truth have also been researched, and are subject to further assessment according to their respective and relative merits.[2][5][6]

Substantive theories of meaning
Correspondence theory
Correspondence theories emphasise that true beliefs and true statements of meaning correspond to the actual state of affairs and that associated meanings must be in agreement with these beliefs and statements.[7] This type of theory stresses a relationship between thoughts or statements on one hand, and things or objects on the other. It is a traditional model tracing its origins to ancient Greek philosophers such as Socrates, Plato, and Aristotle.[8] This class of theories holds that the truth or the falsity of a representation is determined in principle entirely by how it relates to "things", by whether it accurately describes those "things". An example of correspondence theory is the statement by the thirteenth-century philosopher/theologian Thomas Aquinas: Veritas est adaequatio rei et intellectus ("Truth is the equation [or adequation] of things and intellect"), a statement which Aquinas attributed to the ninth-century neoplatonist Isaac Israeli.[9][10][11] Aquinas also restated the theory as: "A judgment is said to be true when it conforms to the external reality".[12]

Correspondence theory centres heavily around the assumption that truth and meaning are a matter of accurately copying what is known as "objective reality" and then representing it in thoughts, words and other symbols.[13] Many modern theorists have stated that this ideal cannot be achieved without analysing additional factors.[2][14] For example, language plays a role in that all languages have words to represent concepts that are virtually undefined in other languages. The German word Zeitgeist is one such example: one who speaks or understands the language may "know" what it means, but any translation of the word apparently fails to accurately capture its full meaning (this is a problem with many abstract words, especially those derived in agglutinative languages). Thus, some words add an additional parameter to the construction of an accurate truth predicate. Among the philosophers who grappled with this problem is Alfred Tarski, whose semantic theory is summarized further below in this article.[15]

Coherence theory
For coherence theories in general, the assessment of meaning and truth requires a proper fit of elements within a whole system. Very often, though, coherence is taken to imply something more than simple logical consistency; often there is a demand that the propositions in a coherent system lend mutual inferential support to each other. So, for example, the completeness and comprehensiveness of the underlying set of concepts is a critical factor in judging the validity and usefulness of a coherent system.[16] A pervasive tenet of coherence theories is the idea that truth is primarily a property of whole systems of propositions, and can be ascribed to individual propositions only according to their coherence with the whole. Among the assortment of perspectives commonly regarded as coherence theory, theorists differ on the question of whether coherence entails many possible true systems of thought or only a single absolute system.

Some variants of coherence theory are claimed to describe the essential and intrinsic properties of formal systems in logic and mathematics.[17] However, formal reasoners are content to contemplate axiomatically independent and sometimes mutually contradictory systems side by side, for example, the various alternative geometries. On the whole, coherence theories have been rejected for lacking justification in their application to other areas of truth, especially with respect to assertions about the natural world, empirical data in general, assertions about practical matters of psychology and society, especially when used without support from the other major theories of truth.[18]

Coherence theories distinguish the thought of rationalist philosophers, particularly of Spinoza, Leibniz, and G.W.F. Hegel, along with the British philosopher F.H. Bradley.[19] Other alternatives may be found among several proponents of logical positivism, notably Otto Neurath and Carl Hempel.

Constructivist theory
Main article: Constructivist epistemology
Social constructivism holds that meaning and truth are constructed by social processes, are historically and culturally specific, and are in part shaped through power struggles within a community. Constructivism views all of our knowledge as "constructed", because it does not reflect any external "transcendent" realities (as a pure correspondence theory might hold). Rather, perceptions of truth are viewed as contingent on convention, human perception, and social experience. It is believed by constructivists that representations of physical and biological reality, including race, sexuality, and gender, are socially constructed.

Giambattista Vico was among the first to claim that history and culture, along with their meaning, are human products. Vico's epistemological orientation gathers the most diverse rays and unfolds in one axiom – verum ipsum factum – "truth itself is constructed". Hegel and Marx were among the other early proponents of the premise that truth is, or can be, socially constructed. Marx, like many critical theorists who followed, did not reject the existence of objective truth but rather distinguished between true knowledge and knowledge that has been distorted through power or ideology. For Marx, scientific and true knowledge is "in accordance with the dialectical understanding of history" and ideological knowledge is "an epiphenomenal expression of the relation of material forces in a given economic arrangement".[20]

Consensus theory
Main article: Consensus theory of truth
Consensus theory holds that meaning and truth are whatever is agreed upon—or, in some versions, might come to be agreed upon—by some specified group. Such a group might include all human beings, or a subset thereof consisting of more than one person.

Among the current advocates of consensus theory as a useful accounting of the concept of "truth" is the philosopher Jürgen Habermas.[21] Habermas maintains that truth is what would be agreed upon in an ideal speech situation.[22] Among the current strong critics of consensus theory is the philosopher Nicholas Rescher.[23]

Pragmatic theory
Main article: Pragmatic theory of truth
The three most influential forms of the pragmatic theory of truth and meaning were introduced around the turn of the 20th century by Charles Sanders Peirce, William James, and John Dewey. Although there are wide differences in viewpoint among these and other proponents of pragmatic theory, they hold in common that meaning and truth are verified and confirmed by the results of putting one's concepts into practice.[24]

Peirce defines truth as follows: "Truth is that concordance of an abstract statement with the ideal limit towards which endless investigation would tend to bring scientific belief, which concordance the abstract statement may possess by virtue of the confession of its inaccuracy and one-sidedness, and this confession is an essential ingredient of truth."[25] This statement stresses Peirce's view that ideas of approximation, incompleteness, and partiality, what he describes elsewhere as fallibilism and "reference to the future", are essential to a proper conception of meaning and truth. Although Peirce uses words like concordance and correspondence to describe one aspect of the pragmatic sign relation, he is also quite explicit in saying that definitions of truth based on mere correspondence are no more than nominal definitions, which he accords a lower status than real definitions.

William James's version of pragmatic theory, while complex, is often summarized by his statement that "the 'true' is only the expedient in our way of thinking, just as the 'right' is only the expedient in our way of behaving".[26] By this, James meant that truth is a quality, the value of which is confirmed by its effectiveness when applying concepts to practice (thus, "pragmatic").

John Dewey, less broadly than James but more broadly than Peirce, held that inquiry, whether scientific, technical, sociological, philosophical or cultural, is self-corrective over time if openly submitted for testing by a community of inquirers in order to clarify, justify, refine and/or refute proposed meanings and truths.[27]

A later variation of the pragmatic theory was William Ernest Hocking's "negative pragmatism": what works may or may not be true, but what fails cannot be true, because the truth and its meaning always works.[28] James's and Dewey's ideas also ascribe meaning and truth to repeated testing, which is "self-corrective" over time.

Pragmatism and negative pragmatism are also closely aligned with the coherence theory of truth in that any testing should not be isolated but rather incorporate knowledge from all human endeavors and experience. The universe is a whole and integrated system, and testing should acknowledge and account for its diversity. As physicist Richard Feynman said: "if it disagrees with experiment, it is wrong".[29]

Associated theories and commentaries
Some have asserted that meaning is nothing substantially more or less than the truth conditions they involve. For such theories, an emphasis is placed upon reference to actual things in the world to account for meaning, with the caveat that reference more or less explains the greater part (or all) of meaning itself.

Logic and language
The logical positivists argued that the meaning of a statement arose from how it is verified.

Gottlob Frege
In his paper "Über Sinn und Bedeutung" (now usually translated as "On Sense and Reference"), Gottlob Frege argued that proper names present at least two problems in explaining meaning.

Suppose the meaning of a name is the thing it refers to. Sam, then, means a person in the world who is named Sam. But if the object referred to by the name did not exist—i.e., Pegasus—then, according to that theory, it would be meaningless.
Suppose two different names refer to the same object. Hesperus and Phosphorus were the names given to what were considered distinct celestial bodies. It was later shown that they were the same thing (the planet Venus). If the words meant the same thing, then substituting one for the other in a sentence would not result in a sentence that differs in meaning from the original. But in that case, "Hesperus is Phosphorus" would mean the same thing as "Hesperus is Hesperus". This is clearly absurd, since we learn something new and unobvious by the former statement, but not by the latter.
Frege can be interpreted as arguing that it was therefore a mistake to think that the meaning of a name is the thing it refers to. Instead, the meaning must be something else—the "sense" of the word. Two names for the same person, then, can have different senses (or meanings): one referent might be picked out by more than one sense. This sort of theory is called a mediated reference theory. Frege argued that, ultimately, the same bifurcation of meaning must apply to most or all linguistic categories, such as to quantificational expressions like "All boats float".

Bertrand Russell
Logical analysis was further advanced by Bertrand Russell and Alfred North Whitehead in their groundbreaking Principia Mathematica, which attempted to produce a formal language with which the truth of all mathematical statements could be demonstrated from first principles.

Russell differed from Frege greatly on many points, however. He rejected Frege's sense-reference distinction. He also disagreed that language was of fundamental significance to philosophy, and saw the project of developing formal logic as a way of eliminating all of the confusions caused by ordinary language, and hence at creating a perfectly transparent medium in which to conduct traditional philosophical argument. He hoped, ultimately, to extend the proofs of the Principia to all possible true statements, a scheme he called logical atomism. For a while it appeared that his pupil Wittgenstein had succeeded in this plan with his Tractatus Logico-Philosophicus.

Russell's work, and that of his colleague G. E. Moore, developed in response to what they perceived as the nonsense dominating British philosophy departments at the turn of the 20th century, which was a kind of British Idealism most of which was derived (albeit very distantly) from the work of Hegel. In response Moore developed an approach ("Common Sense Philosophy"[30]) which sought to examine philosophical difficulties by a close analysis of the language used in order to determine its meaning. In this way Moore sought to expunge philosophical absurdities such as "time is unreal". Moore's work would have significant, if oblique, influence (largely mediated by Wittgenstein) on Ordinary language philosophy.

Other truth theories of meaning
The Vienna Circle, a famous group of logical positivists from the early 20th century (closely allied with Russell and Frege), adopted the verificationist theory of meaning, a type of truth theory of meaning.[31] The verificationist theory of meaning (in at least one of its forms) states that to say that an expression is meaningful is to say that there are some conditions of experience that could exist to show that the expression is true. As noted, Frege and Russell were two proponents of this way of thinking.

A semantic theory of truth was produced by Alfred Tarski for formal semantics. According to Tarski's account, meaning consists of a recursive set of rules that end up yielding an infinite set of sentences, "'p' is true if and only if p", covering the whole language. His innovation produced the notion of propositional functions discussed on the section on universals (which he called "sentential functions"), and a model-theoretic approach to semantics (as opposed to a proof-theoretic one). Finally, some links were forged to the correspondence theory of truth (Tarski, 1944).

Perhaps the most influential current approach in the contemporary theory of meaning is that sketched by Donald Davidson in his introduction to the collection of essays Truth and Meaning in 1967. There he argued for the following two theses:

Any learnable language must be statable in a finite form, even if it is capable of a theoretically infinite number of expressions—as we may assume that natural human languages are, at least in principle. If it could not be stated in a finite way then it could not be learned through a finite, empirical method such as the way humans learn their languages. It follows that it must be possible to give a theoretical semantics for any natural language which could give the meanings of an infinite number of sentences on the basis of a finite system of axioms.
Giving the meaning of a sentence, he further argued, was equivalent to stating its truth conditions. He proposed that it must be possible to account for language as a set of distinct grammatical features together with a lexicon, and for each of them explain its workings in such a way as to generate trivial (obviously correct) statements of the truth conditions of all the (infinitely many) sentences built up from these.
The result is a theory of meaning that rather resembles, by no accident, Tarski's account.

Davidson's account, though brief, constitutes the first systematic presentation of truth-conditional semantics. He proposed simply translating natural languages into first-order predicate calculus in order to reduce meaning to a function of truth.

Saul Kripke
Saul Kripke examined the relation between sense and reference in dealing with possible and actual situations. He showed that one consequence of his interpretation of certain systems of modal logic was that the reference of a proper name is necessarily linked to its referent, but that the sense is not. So for instance "Hesperus" necessarily refers to Hesperus, even in those imaginary cases and worlds in which perhaps Hesperus is not the evening star. That is, Hesperus is necessarily Hesperus, but only contingently the morning star.

This results in the curious situation that part of the meaning of a name — that it refers to some particular thing — is a necessary fact about that name, but another part — that it is used in some particular way or situation — is not.

Kripke also drew the distinction between speaker's meaning and semantic meaning, elaborating on the work of ordinary language philosophers Paul Grice and Keith Donnellan. The speaker's meaning is what the speaker intends to refer to by saying something; the semantic meaning is what the words uttered by the speaker mean according to the language.

In some cases, people do not say what they mean; in other cases, they say something that is in error. In both these cases, the speaker's meaning and the semantic meaning seem to be different. Sometimes words do not actually express what the speaker wants them to express; so words will mean one thing, and what people intend to convey by them might mean another. The meaning of the expression, in such cases, is ambiguous.

Critiques of truth theories of meaning
W. V. O. Quine attacked both verificationism and the very notion of meaning in his famous essay, "Two Dogmas of Empiricism". In it, he suggested that meaning was nothing more than a vague and dispensable notion. Instead, he asserted, what was more interesting to study was the synonymy between signs. He also pointed out that verificationism was tied to the distinction between analytic and synthetic statements, and asserted that such a divide was defended ambiguously. He also suggested that the unit of analysis for any potential investigation into the world (and, perhaps, meaning) would be the entire body of statements taken as a collective, not just individual statements on their own.

Other criticisms can be raised on the basis of the limitations that truth-conditional theorists themselves admit to. Tarski, for instance, recognized that truth-conditional theories of meaning only make sense of statements, but fail to explain the meanings of the lexical parts that make up statements. Rather, the meaning of the parts of statements is presupposed by an understanding of the truth-conditions of a whole statement, and explained in terms of what he called "satisfaction conditions".

Still another objection (noted by Frege and others) was that some kinds of statements don't seem to have any truth-conditions at all. For instance, "Hello!" has no truth-conditions, because it doesn't even attempt to tell the listener anything about the state of affairs in the world. In other words, different propositions have different grammatical moods.

Deflationist accounts of truth, sometimes called 'irrealist' accounts, are the staunchest source of criticism of truth-conditional theories of meaning. According to them, "truth" is a word with no serious meaning or function in discourse. For instance, for the deflationist, the sentences "It's true that Tiny Tim is trouble" and "Tiny Tim is trouble" are equivalent. In consequence, for the deflationist, any appeal to truth as an account of meaning has little explanatory power.

The sort of truth theories presented here can also be attacked for their formalism both in practice and principle. The principle of formalism is challenged by the informalists, who suggest that language is largely a construction of the speaker, and so, not compatible with formalization. The practice of formalism is challenged by those who observe that formal languages (such as present-day quantificational logic) fail to capture the expressive power of natural languages (as is arguably demonstrated in the awkward character of the quantificational explanation of definite description statements, as laid out by Bertrand Russell).

Finally, over the past century, forms of logic have been developed that are not dependent exclusively on the notions of truth and falsity. Some of these types of logic have been called modal logics. They explain how certain logical connectives such as "if-then" work in terms of necessity and possibility. Indeed, modal logic was the basis of one of the most popular and rigorous formulations in modern semantics called the Montague grammar. The successes of such systems naturally give rise to the argument that these systems have captured the natural meaning of connectives like if-then far better than an ordinary, truth-functional logic ever could.

Usage and meaning
Throughout the 20th century, English philosophy focused closely on analysis of language. This style of analytic philosophy became very influential and led to the development of a wide range of philosophical tools.

Ludwig Wittgenstein
The philosopher Ludwig Wittgenstein was originally an ideal language philosopher, following the influence of Russell and Frege. In his Tractatus Logico-Philosophicus he had supported the idea of an ideal language built up from atomic statements using logical connectives (see picture theory of meaning and logical atomism). However, as he matured, he came to appreciate more and more the phenomenon of natural language. Philosophical Investigations, published after his death, signalled a sharp departure from his earlier work with its focus upon ordinary language use (see use theory of meaning and ordinary language philosophy). His approach is often summarised by the aphorism "the meaning of a word is its use in a language". However, following in Frege's footsteps, in the Tractatus, Wittgenstein declares: "... Only in the context of a proposition has a name meaning."[32]

His work would come to inspire future generations and spur forward a whole new discipline, which explained meaning in a new way. Meaning in a natural language was seen as primarily a question of how the speaker uses words within the language to express intention.

This close examination of natural language proved to be a powerful philosophical technique. Practitioners who were influenced by Wittgenstein's approach have included an entire tradition of thinkers, featuring P. F. Strawson, Paul Grice, R. M. Hare, R. S. Peters, and Jürgen Habermas.

J. L. Austin
At around the same time Ludwig Wittgenstein was re-thinking his approach to language, reflections on the complexity of language led to a more expansive approach to meaning. Following the lead of George Edward Moore, J. L. Austin examined the use of words in great detail. He argued against fixating on the meaning of words. He showed that dictionary definitions are of limited philosophical use, since there is no simple "appendage" to a word that can be called its meaning. Instead, he showed how to focus on the way in which words are used in order to do things. He analysed the structure of utterances into three distinct parts: locutions, illocutions and perlocutions. His pupil John Searle developed the idea under the label "speech acts". Their work greatly influenced pragmatics.

Peter Strawson
Past philosophers had understood reference to be tied to words themselves. However, Peter Strawson disagreed in his seminal essay, "On Referring", where he argued that there is nothing true about statements on their own; rather, only the uses of statements could be considered to be true or false.

Indeed, one of the hallmarks of the ordinary use perspective is its insistence upon the distinctions between meaning and use. "Meanings", for ordinary language philosophers, are the instructions for usage of words — the common and conventional definitions of words. Usage, on the other hand, is the actual meanings that individual speakers have — the things that an individual speaker in a particular context wants to refer to. The word "dog" is an example of a meaning, but pointing at a nearby dog and shouting "This dog smells foul!" is an example of usage. From this distinction between usage and meaning arose the divide between the fields of pragmatics and semantics.

Yet another distinction is of some utility in discussing language: "mentioning". Mention is when an expression refers to itself as a linguistic item, usually surrounded by quotation marks. For instance, in the expression "'Opopanax' is hard to spell", what is referred to is the word itself ("opopanax") and not what it means (an obscure gum resin). Frege had referred to instances of mentioning as "opaque contexts".

In his essay, "Reference and Definite Descriptions", Keith Donnellan sought to improve upon Strawson's distinction. He pointed out that there are two uses of definite descriptions: attributive and referential. Attributive uses provide a description of whoever is being referred to, while referential uses point out the actual referent. Attributive uses are like mediated references, while referential uses are more directly referential.

Paul Grice
The philosopher Paul Grice, working within the ordinary language tradition, understood "meaning" — in his 1957 article — to have two kinds: natural and non-natural. Natural meaning had to do with cause and effect, for example with the expression "these spots mean measles". Non-natural meaning, on the other hand, had to do with the intentions of the speaker in communicating something to the listener.

In his essay, Logic and Conversation, Grice went on to explain and defend an explanation of how conversations work. His guiding maxim was called the cooperative principle, which claimed that the speaker and the listener will have mutual expectations of the kind of information that will be shared. The principle is broken down into four maxims: Quality (which demands truthfulness and honesty), Quantity (demand for just enough information as is required), Relation (relevance of things brought up), and Manner (lucidity). This principle, if and when followed, lets the speaker and listener figure out the meaning of certain implications by way of inference.

The works of Grice led to an avalanche of research and interest in the field, both supportive and critical. One spinoff was called Relevance theory, developed by Dan Sperber and Deirdre Wilson during the mid-1980s, whose goal was to make the notion of relevance more clear. Similarly, in his work, "Universal pragmatics", Jürgen Habermas began a program that sought to improve upon the work of the ordinary language tradition. In it, he laid out the goal of a valid conversation as a pursuit of mutual understanding.

Noam Chomsky
Although he has focused on the structure and functioning of human syntax, in many works[33][34][35][36] Noam Chomsky has discussed many philosophical problems too, including the problem of meaning and reference in human language. Chomsky has formulated a strong criticism against both the externalist notion of reference (reference consists in a direct or causal relation among words and objects) and the internalist one (reference is a mind-mediated relation holding among words and reality). According to Chomsky, both these notions (and many others widely used in philosophy, such as that of truth) are basically inadequate for the naturalistic (= scientific) inquiry on human mind: they are common sense notions, not scientific notions, which cannot, as such, enter in the scientific discussion. Chomsky argues that the notion of reference can be used only when we deal with scientific languages, whose symbols refers to specific things or entities; but when we consider human language expressions, we immediately understand that their reference is vague, in the sense that they can be used to denote many things. For example, the word “book” can be used to denote an abstract object (e.g., “he is reading the book”) or a concrete one (e.g., “the book is on the chair”); the name “London” can denote at the same time a set of buildings, the air of a place and the character of a population (think to the sentence “London is so gray, polluted and sad”). These and other cases induce Chomsky to argue that the only plausible (although not scientific) notion of reference is that of act of reference, a complex phenomenon of language use (performance) which includes many factors (linguistic and not: i.e. beliefs, desires, assumptions about the world, premises, etc.). As Chomsky himself has pointed out, [37] this conception of meaning is very close to that adopted by John Austin, Peter Strawson and the late Wittgenstein.[38]

Inferential role semantics
Main article: Inferential role semantics
Michael Dummett argued against the kind of truth-conditional semantics presented by Davidson. Instead, he argued that basing semantics on assertion conditions avoids a number of difficulties with truth-conditional semantics, such as the transcendental nature of certain kinds of truth condition. He leverages work done in proof-theoretic semantics to provide a kind of inferential role semantics, where:

The meaning of sentences and grammatical constructs is given by their assertion conditions; and
Such a semantics is only guaranteed to be coherent if the inferences associated with the parts of language are in logical harmony.
A semantics based upon assertion conditions is called a verificationist semantics: cf. the verificationism of the Vienna Circle.

This work is closely related, though not identical, to one-factor theories of conceptual role semantics.

Critiques of use theories of meaning
Sometimes between the 1950-1990s, cognitive scientist Jerry Fodor said that use theories of meaning (of the Wittgensteinian kind) seem to assume that language is solely a public phenomenon, that there is no such thing as a "private language". Fodor thinks it is necessary to create or describe the language of thought, which would seemingly require the existence of a "private language".

In the 1960s, David Kellogg Lewis described meaning as use, a feature of a social convention and conventions as regularities of a specific sort. Lewis' work was an application of game theory in philosophical topics.[39] Conventions, he argued, are a species of coordination equilibria.

Idea theory of meaning

Memberships of a graded class
The idea theory of meaning (also ideational theory of meaning), most commonly associated with the British empiricist John Locke, claims that meanings are mental representations provoked by signs.[40]

The term "ideas" is used to refer to either mental representations, or to mental activity in general. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of mind than the latter. Those who seek an explanation for meaning in the former sort of account endorse a stronger sort of idea theory of meaning than the latter.

Each idea is understood to be necessarily about something external and/or internal, real or imaginary. For example, in contrast to the abstract meaning of the universal "dog", the referent "this dog" may mean a particular real life chihuahua. In both cases, the word is about something, but in the former it is about the class of dogs as generally understood, while in the latter it is about a very real and particular dog in the real world.

John Locke considered all ideas to be both imaginable objects of sensation and the very unimaginable objects of reflection. He said in his Essay Concerning Human Understanding that words are used both as signs for ideas and also to signify a lack of certain ideas. David Hume held that thoughts were kinds of imaginable entities: his Enquiry Concerning Human Understanding, section 2. He argued that any words that could not call upon any past experience were without meaning.

In contrast to Locke and Hume, George Berkeley and Ludwig Wittgenstein held that ideas alone are unable to account for the different variations within a general meaning. For example, any hypothetical image of the meaning of "dog" has to include such varied images as a chihuahua, a pug, and a black Labrador; and this seems impossible to imagine, since all of those particular breeds look very different from one another. Another way to see this point is to question why it is that, if we have an image of a specific type of dog (say of a chihuahua), it should be entitled to represent the entire concept.

Another criticism is that some meaningful words, known as non-lexical items, don't have any meaningfully associated image. For example, the word "the" has a meaning, but one would be hard-pressed to find a mental representation that fits it. Still another objection lies in the observation that certain linguistic items name something in the real world, and are meaningful, yet which we have no mental representations to deal with. For instance, it is not known what Newton's father looked like, yet the phrase "Newton's father" still has meaning.

Another problem is that of composition—that it is difficult to explain how words and phrases combine into sentences if only ideas are involved in meaning.

Eleanor Rosch and George Lakoff have advanced a theory of "prototypes" which suggests that many lexical categories, at least on the face of things, have "radial structures". That is to say, there are some ideal member(s) in the category that seem to represent the category better than other members. For example, the category of "birds" may feature the robin as the prototype, or the ideal kind of bird. With experience, subjects might come to evaluate membership in the category of "bird" by comparing candidate members to the prototype and evaluating for similarities. So, for example, a penguin or an ostrich would sit at the fringe of the meaning of "bird", because a penguin is unlike a robin.

Intimately related to these researches is the notion of a psychologically basic level, which is both the first level named and understood by children, and "the highest level at which a single mental image can reflect the entire category" (Lakoff 1987:46). The "basic level" of cognition is understood by Lakoff as crucially drawing upon "image-schemas" along with various other cognitive processes.

Philosophers Ned Block, Gilbert Harman and Hartry Field, and cognitive scientists G. Miller and P. Johnson-Laird say that the meaning of a term can be found by investigating its role in relation to other concepts and mental states. They endorse a "conceptual role semantics". Those proponents of this view who understand meanings to be exhausted by the content of mental states can be said to endorse "one-factor" accounts of conceptual role semantics and thus to fit within the tradition of idea theories.

Emotions are biological states associated with all of the nerve systems[1][2][3] brought on by neurophysiological changes variously associated with thoughts, feelings, behavioural responses, and a degree of pleasure or displeasure.[4][5] There is currently no scientific consensus on a definition. Emotions are often intertwined with mood, temperament, personality, disposition, creativity,[6] and motivation.[7]

Research on emotion has increased significantly over the past two decades with many fields contributing including psychology, neuroscience, affective neuroscience, endocrinology, medicine, history, sociology of emotions, and computer science. The numerous theories that attempt to explain the origin, neurobiology, experience, and function of emotions have only fostered more intense research on this topic. Current areas of research in the concept of emotion include the development of materials that stimulate and elicit emotion. In addition, PET scans and fMRI scans help study the affective picture processes in the brain.[8]

From a purely mechanistic perspective, emotions can be defined as "a positive or negative experience that is associated with a particular pattern of physiological activity." Emotions produce different physiological, behavioral and cognitive changes. The original role of emotions was to motivate adaptive behaviors that in the past would have contributed to the passing on of genes through survival, reproduction, and kin selection.[9][10]

In some theories, cognition is an important aspect of emotion. For those who act primarily on emotions, they may assume that they are not thinking, but mental processes involving cognition are still essential, particularly in the interpretation of events. For example, the realization of our believing that we are in a dangerous situation and the subsequent arousal of our body's nervous system (rapid heartbeat and breathing, sweating, muscle tension) is integral to the experience of our feeling afraid. Other theories, however, claim that emotion is separate from and can precede cognition. Consciously experiencing an emotion is exhibiting a mental representation of that emotion from a past or hypothetical experience, which is linked back to a content state of pleasure or displeasure.[11] The content states are established by verbal explanations of experiences, describing an internal state.[12]

Emotions are complex. According to some theories, they are states of feeling that result in physical and psychological changes that influence our behavior.[5] The physiology of emotion is closely linked to arousal of the nervous system with various states and strengths of arousal relating, apparently, to particular emotions. Emotion is also linked to behavioral tendency. Extroverted people are more likely to be social and express their Knowledge is a familiarity, awareness, or understanding of someone or something, such as facts (propositional knowledge), skills (procedural knowledge), or objects (acquaintance knowledge). By most accounts, knowledge can be acquired in many different ways and from many sources, including but not limited to perception, reason, memory, testimony, scientific inquiry, education, and practice. The philosophical study of knowledge is called epistemology.

The term "knowledge" can refer to a theoretical or practical understanding of a subject. It can be implicit (as with practical skill or expertise) or explicit (as with the theoretical understanding of a subject); formal or informal; systematic or particular.[1] The philosopher Plato famously pointed out the need for a distinction between knowledge and true belief in the Theaetetus, leading many to attribute to him a definition of knowledge as "justified true belief".[2][3] The difficulties with this definition raised by the Gettier problem have been the subject of extensive debate in epistemology for more than half a century.[2]

Robert Reid, Knowledge (1896). Thomas Jefferson Building, Washington, D.C.
Main article: Epistemology
The eventual demarcation of philosophy from science was made possible by the notion that philosophy's core was "theory of knowledge," a theory distinct from the sciences because it was their foundation... Without this idea of a "theory of knowledge," it is hard to imagine what "philosophy" could have been in the age of modern science.

— Richard Rorty, Philosophy and the Mirror of Nature
Knowledge is the primary subject of the field of epistemology, which studies what we know, how we come to know it, and what it means to know something.[4]

The definition of knowledge is a matter of ongoing debate among epistemologists. The classical definition, described but not ultimately endorsed by Plato,[5] specifies that a statement must meet three criteria in order to be considered knowledge: it must be justified, true, and believed. Epistemologists today generally agree that these conditions are not sufficient, as various Gettier cases are thought to demonstrate. There are a number of alternative definitions which have been proposed, including Robert Nozick's proposal that all instances of knowledge must 'track the truth' and Simon Blackburn's proposal that those who have a justified true belief 'through a defect, flaw, or failure' fail to have knowledge. Richard Kirkham suggests that our definition of knowledge requires that the evidence for the belief necessitates its truth.[6]

In contrast to this approach, Ludwig Wittgenstein observed, following Moore's paradox, that one can say "He believes it, but it isn't so," but not "He knows it, but it isn't so."[7] He goes on to argue that these do not correspond to distinct mental states, but rather to distinct ways of talking about conviction. What is different here is not the mental state of the speaker, but the activity in which they are engaged. For example, on this account, to know that the kettle is boiling is not to be in a particular state of mind, but to perform a particular task with the statement that the kettle is boiling. Wittgenstein sought to bypass the difficulty of definition by looking to the way "knowledge" is used in natural languages. He saw knowledge as a case of a family resemblance. Following this idea, "knowledge" has been reconstructed as a cluster concept that points out relevant features but that is not adequately captured by any definition.[8]

Self-knowledge
“Self-knowledge” usually refers to a person's knowledge of their own sensations, thoughts, beliefs, and other mental states.[9] A number of questions regarding self-knowledge have been the subject of extensive debates in philosophy, including whether self-knowledge differs from other types of knowledge, whether we have privileged self-knowledge compared to knowledge of other minds, and the nature of our acquaintance with ourselves.[9] David Hume famously expressed skepticism about whether we could ever have self-knowledge over and above our immediate awareness of a "bundle of perceptions", which was part of his broader skepticism about personal identity.[9]

The value of knowledge

Los portadores de la antorcha (The Torch-Bearers) – Sculpture by Anna Hyatt Huntington symbolizing the transmission of knowledge from one generation to the next (Ciudad Universitaria, Madrid, Spain)
It is generally assumed that knowledge is more valuable than mere true belief. If so, what is the explanation? A formulation of the value problem in epistemology first occurs in Plato's Meno. Socrates points out to Meno that a man who knew the way to Larissa could lead others there correctly. But so, too, could a man who had true beliefs about how to get there, even if he had not gone there or had any knowledge of Larissa. Socrates says that it seems that both knowledge and true opinion can guide action. Meno then wonders why knowledge is valued more than true belief and why knowledge and true belief are different. Socrates responds that knowledge is more valuable than mere true belief because it is tethered or justified. Justification, or working out the reason for a true belief, locks down true belief.[10]

The problem is to identify what (if anything) makes knowledge more valuable than mere true belief, or that makes knowledge more valuable than a mere minimal conjunction of its components, such as justification, safety, sensitivity, statistical likelihood, and anti-Gettier conditions, on a particular analysis of knowledge that conceives of knowledge as divided into components (to which knowledge-first epistemological theories, which posit knowledge as fundamental, are notable exceptions).[11] The value problem re-emerged in the philosophical literature on epistemology in the twenty-first century following the rise of virtue epistemology in the 1980s, partly because of the obvious link to the concept of value in ethics.[12]

In contemporary philosophy, epistemologists including Ernest Sosa, John Greco, Jonathan Kvanvig,[13] Linda Zagzebski, and Duncan Pritchard have defended virtue epistemology as a solution to the value problem. They argue that epistemology should also evaluate the "properties" of people as epistemic agents (i.e. intellectual virtues), rather than merely the properties of propositions and propositional mental attitudes.

Scientific knowledge
Main article: Philosophy of science

Sir Francis Bacon, "Knowledge is Power"
The development of the scientific method has made a significant contribution to how knowledge of the physical world and its phenomena is acquired.[14] To be termed scientific, a method of inquiry must be based on gathering observable and measurable evidence subject to specific principles of reasoning and experimentation.[15] The scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses.[16] Science, and the nature of scientific knowledge have also become the subject of philosophy. As science itself has developed, scientific knowledge now includes a broader usage[17] in the soft sciences such as biology and the social sciences – discussed elsewhere as meta-epistemology, or genetic epistemology, and to some extent related to "theory of cognitive development". Note that "epistemology" is the study of knowledge and how it is acquired. Science is "the process used everyday to logically complete thoughts through inference of facts determined by calculated experiments." Sir Francis Bacon was critical in the historical development of the scientific method; his works established and popularized an inductive methodology for scientific inquiry. His famous aphorism, "knowledge is power", is found in the Meditations Sacrae (1597).[18]

Until recent times, at least in the Western tradition, it was simply taken for granted that knowledge was something possessed only by humans – and probably adult humans at that. Sometimes the notion might stretch to Society-as-such, as in (e. g.) "the knowledge possessed by the Coptic culture" (as opposed to its individual members), but that was not assured either. Nor was it usual to consider unconscious knowledge in any systematic way until this approach was popularized by Freud.[19]

Other biological domains where "knowledge" might be said to reside, include: (iii) the immune system, and (iv) in the DNA of the genetic code. See the list of four "epistemological domains": Popper, (1975);[20] and Traill (2008:[21] Table S, p. 31) – also references by both to Niels Jerne.

Such considerations seem to call for a separate definition of "knowledge" to cover the biological systems. For biologists, knowledge must be usefully available to the system, though that system need not be conscious. Thus the criteria seem to be:

The system should apparently be dynamic and self-organizing (unlike a mere book on its own).
The knowledge must constitute some sort of representation of "the outside world",[22] or ways of dealing with it (directly or indirectly).
Some way must exist for the system to access this information quickly enough for it to be useful.
Those who use the phrase "scientific knowledge" don't necessary claim to certainty, since scientists will never be absolutely certain when they are correct and when they are not. It is thus an irony of proper scientific method that one must doubt even when correct, in the hopes that this practice will lead to greater convergence on the truth in general.[23]

Situated knowledge
"Situated knowledges" redirects here. For the Donna Haraway essay, see Situated Knowledges.
Situated knowledge is knowledge specific to a particular situation. It was used by Donna Haraway as an extension of the feminist approaches of "successor science" suggested by Sandra Harding, one which "offers a more adequate, richer, better account of a world, in order to live in it well and in critical, reflexive relation to our own as well as others' practices of domination and the unequal parts of privilege and oppression that makes up all positions."[24] This situation partially transforms science into a narrative, which Arturo Escobar explains as, "neither fictions nor supposed facts." This narrative of situation is historical textures woven of fact and fiction, and as Escobar explains further, "even the most neutral scientific domains are narratives in this sense," insisting that rather than a purpose dismissing science as a trivial matter of contingency, "it is to treat (this narrative) in the most serious way, without succumbing to its mystification as 'the truth' or to the ironic skepticism common to many critiques."[25]

Haraway's argument stems from the limitations of the human perception, as well as the overemphasis of the sense of vision in science. According to Haraway, vision in science has been, "used to signify a leap out of the marked body and into a conquering gaze from nowhere." This is the "gaze that mythically inscribes all the marked bodies, that makes the unmarked category claim the power to see and not be seen, to represent while escaping representation."[24] This causes a limitation of views in the position of science itself as a potential player in the creation of knowledge, resulting in a position of "modest witness". This is what Haraway terms a "god trick", or the aforementioned representation while escaping representation.[26] In order to avoid this, "Haraway perpetuates a tradition of thought which emphasizes the importance of the subject in terms of both ethical and political accountability".[27]

Some methods of generating knowledge, such as trial and error, or learning from experience, tend to create highly situational knowledge. Situational knowledge is often embedded in language, culture, or traditions. This integration of situational knowledge is an allusion to the community, and its attempts at collecting subjective perspectives into an embodiment "of views from somewhere." [24] Knowledge is also said to be related to the capacity of acknowledgement in human beings.[28]

Even though Haraway's arguments are largely based on feminist studies,[24] this idea of different worlds, as well as the skeptic stance of situated knowledge is present in the main arguments of post-structuralism. Fundamentally, both argue the contingency of knowledge on the presence of history; power, and geography, as well as the rejection of universal rules or laws or elementary structures; and the idea of power as an inherited trait of objectification.[29]

Partial knowledge

This section does not cite any sources. Please help improve this section by adding citations to reliable sources. Unsourced material may be challenged and removed. (June 2020) (Learn how and when to remove this template message)

The parable of the blind men and the elephant suggests that people tend to project their partial experiences as the whole truth
One discipline of epistemology focuses on partial knowledge. In most cases, it is not possible to understand an information domain exhaustively; our knowledge is always incomplete or partial. Most real problems have to be solved by taking advantage of a partial understanding of the problem context and problem data, unlike the typical math problems one might solve at school, where all data is given and one is given a complete understanding of formulas necessary to solve them (False consensus effect).

This idea is also present in the concept of bounded rationality which assumes that in real-life situations people often have a limited amount of information and make decisions accordingly.

Religious accounts of knowledge
In many expressions of Christianity, such as Catholicism and Anglicanism, knowledge is one of the seven gifts of the Holy Spirit.[30]

The Old Testament's tree of the knowledge of good and evil contained the knowledge that separated Man from God: "And the LORD God said, Behold, the man is become as one of us, to know good and evil..." (Genesis 3:22)

In Gnosticism, divine knowledge or gnosis is hoped to be attained.

विद्या दान (Vidya Daan) i.e. knowledge sharing is a major part of Daan, a tenet of all Dharmic Religions.[31] Hindu Scriptures present two kinds of knowledge, Paroksh Gyan and Prataksh Gyan. Paroksh Gyan (also spelled Paroksha-Jnana) is secondhand knowledge: knowledge obtained from books, hearsay, etc. Pratyaksh Gyan (also spelled Pratyaksha-Jnana) is the knowledge borne of direct experience, i.e., knowledge that one discovers for oneself.[32] Jnana yoga ("path of knowledge") is one of three main types of yoga expounded by Krishna in the Bhagavad Gita. (It is compared and contrasted with Bhakti Yoga and Karma yoga.)

In Islam, knowledge (Arabic: علم, ʿilm) is given great significance. "The Knowing" (al-ʿAlīm) is one of the 99 names reflecting distinct attributes of God. The Qur'an asserts that knowledge comes from God (2:239) and various hadith encourage the acquisition of knowledge. Muhammad is reported to have said "Seek knowledge from the cradle to the grave" and "Verily the men of knowledge are the inheritors of the prophets". Islamic scholars, theologians and jurists are often given the title alim, meaning "knowledgeble".[33]

In Jewish tradition, knowledge (Hebrew: דעת da'ath) is considered one of the most valuable traits a person can acquire. Observant Jews recite three times a day in the Amidah "Favor us with knowledge, understanding and discretion that come from you. Exalted are you, Existent-One, the gracious giver of knowledge." The Tanakh states, "A wise man gains power, and a man of knowledge maintains power", and "knowledge is chosen above gold".
emotions, while introverted people are more likely to be more socially withdrawn and conceal their emotions. Emotion is often the driving force behind motivation, positive or negative.[13] According to other theories, emotions are not causal forces but simply syndromes of components, which might include motivation, feeling, behavior, and physiological changes, but no one of these components is the emotion. Nor is the emotion an entity that causes these components.[14]

Emotions involve different components, such as subjective experience, cognitive processes, expressive behavior, psychophysiological changes, and instrumental behavior. At one time, academics attempted to identify the emotion with one of the components: William James with a subjective experience, behaviorists with instrumental behavior, psychophysiologists with physiological changes, and so on. More recently, emotion is said to consist of all the components. The different components of emotion are categorized somewhat differently depending on the academic discipline. In psychology and philosophy, emotion typically includes a subjective, conscious experience characterized primarily by psychophysiological expressions, biological reactions, and mental states. A similar multi-componential description of emotion is found in sociology. For example, Peggy Thoits[15] described emotions as involving physiological components, cultural or emotional labels (anger, surprise, etc.), expressive body actions, and the appraisal of situations and contexts.

Human nature and the following bodily sensations have been always part of the interest of thinkers and philosophers. Far most extensively, this interest has been of great interest by both Western and Eastern societies. Emotional states have been associated with the divine and the enlightenment of the human mind and body.[16] The ever changing actions of individuals and its mood variations have been of great importance by most of the Western philosophers (Aristotle, Plato, Descartes, Aquinas, Hobbes) that lead them to propose vast theories; often competing theories, that sought to explain emotion and the following motivators of human action and its consequences.

In the Age of Enlightenment Scottish thinker David Hume[17] proposed a revolutionary argument that sought to explain the main motivators of human action and conduct. He proposed that actions are motivated by "fears, desires, and passions". As he wrote in his book Treatise of Human Nature (1773): "Reason alone can never be a motive to any action of the will… it can never oppose passion in the direction of the will… Reason is, and ought to be the slave of the passions, and can never pretend to any other office than to serve and obey them".[18] With these lines Hume pretended to explain that reason and further action will be subjected to the desires and experience of the self. Later thinkers would propose that actions and emotions are deeply interrelated to social, political, historical, and cultural aspects of reality that would be also associated with sophisticated neurological and physiological research on the brain and other parts of the physical body.

Etymology

Sixteen faces expressing the human passions-coloured engraving by J. Pass, 1821, after Charles Le Brun
The word "emotion" dates back to 1579, when it was adapted from the French word émouvoir, which means "to stir up". The term emotion was introduced into academic discussion as a catch-all term to passions, sentiments and affections.[19] The word "emotion" was coined in the early 1800s by Thomas Brown and it is around the 1830s that the modern concept of emotion first emerged for the English language.[20] "No one felt emotions before about 1830. Instead they felt other things - "passions", "accidents of the soul", "moral sentiments" - and explained them very differently from how we understand emotions today."[20]

Some cross-cultural studies indicate that the categorization of "emotion" and classification of basic emotions such as "anger" and "sadness" are not universal and that the boundaries and domains of these concepts are categorized differently by all cultures.[21] However, others argue that there are some universal bases of emotions (see Section 6.1).[22] In psychiatry and psychology, an inability to express or perceive emotion is sometimes referred to as alexithymia.[23]

Definitions
The Oxford Dictionaries definition of emotion is "A strong feeling deriving from one's circumstances, mood, or relationships with others."[24] Emotions are responses to significant internal and external events.[25]

Emotions can be occurrences (e.g., panic) or dispositions (e.g., hostility), and short-lived (e.g., anger) or long-lived (e.g., grief).[26] Psychotherapist Michael C. Graham describes all emotions as existing on a continuum of intensity.[27] Thus fear might range from mild concern to terror or shame might range from simple embarrassment to toxic shame.[28] Emotions have been described as consisting of a coordinated set of responses, which may include verbal, physiological, behavioral, and neural mechanisms.[29]

Emotions have been categorized, with some relationships existing between emotions and some direct opposites existing. Graham differentiates emotions as functional or dysfunctional and argues all functional emotions have benefits.[30]

In some uses of the word, emotions are intense feelings that are directed at someone or something.[31] On the other hand, emotion can be used to refer to states that are mild (as in annoyed or content) and to states that are not directed at anything (as in anxiety and depression). One line of research looks at the meaning of the word emotion in everyday language and finds that this usage is rather different from that in academic discourse.[32]

In practical terms, Joseph LeDoux has defined emotions as the result of a cognitive and conscious process which occurs in response to a body system response to a trigger.[33]

Components
According to Scherer's Component Process Model (CPM) of emotion,[34] there are five crucial elements of emotion. From the component process perspective, emotional experience requires that all of these processes become coordinated and synchronized for a short period of time, driven by appraisal processes. Although the inclusion of cognitive appraisal as one of the elements is slightly controversial, since some theorists make the assumption that emotion and cognition are separate but interacting systems, the CPM provides a sequence of events that effectively describes the coordination involved during an emotional episode.

Cognitive appraisal: provides an evaluation of events and objects.
Bodily symptoms: the physiological component of emotional experience.
Action tendencies: a motivational component for the preparation and direction of motor responses.
Expression: facial and vocal expression almost always accompanies an emotional state to communicate reaction and intention of actions.
Feelings: the subjective experience of emotional state once it has occurred.
Differentiation
See also: Affect measures § Differentiating affect from other terms
Emotion can be differentiated from a number of similar constructs within the field of affective neuroscience:[29]

Feeling; not all feelings include emotion, such as the feeling of knowing. In the context of emotion, feelings are best understood as a subjective representation of emotions, private to the individual experiencing them.[35][better source needed]
Moods are diffuse affective states that generally last for much longer durations than emotions, are also usually less intense than emotions and often appear to lack a contextual stimulus.[31]
Affect is used to describe the underlying affective experience of an emotion or a mood.
Purpose and value
One view is that emotions facilitate adaptive responses to environmental challenges. Emotions have been described as a result of evolution because they provided good solutions to ancient and recurring problems that faced our ancestors.[36] Emotions can function as a way to communicate what's important to us, such as values and ethics.[37] However some emotions, such as some forms of anxiety, are sometimes regarded as part of a mental illness and thus possibly of negative value.[38]

Classification
Main article: Emotion classification
A distinction can be made between emotional episodes and emotional dispositions. Emotional dispositions are also comparable to character traits, where someone may be said to be generally disposed to experience certain emotions. For example, an irritable person is generally disposed to feel irritation more easily or quickly than others do. Finally, some theorists place emotions within a more general category of "affective states" where affective states can also include emotion-related phenomena such as pleasure and pain, motivational states (for example, hunger or curiosity), moods, dispositions and traits.[39]

Basic emotions

Examples of basic emotions

The emotion wheel.
For more than 40 years, Paul Ekman has supported the view that emotions are discrete, measurable, and physiologically distinct. Ekman's most influential work revolved around the finding that certain emotions appeared to be universally recognized, even in cultures that were preliterate and could not have learned associations for facial expressions through media. Another classic study found that when participants contorted their facial muscles into distinct facial expressions (for example, disgust), they reported subjective and physiological experiences that matched the distinct facial expressions. Ekman's facial-expression research examined six basic emotions: anger, disgust, fear, happiness, sadness and surprise.[40] Later in his career,[41] Ekman theorized that other universal emotions may exist beyond these six. In light of this, recent cross-cultural studies led by Daniel Cordaro and Dacher Keltner, both former students of Ekman, extended the list of universal emotions. In addition to the original six, these studies provided evidence for amusement, awe, contentment, desire, embarrassment, pain, relief, and sympathy in both facial and vocal expressions. They also found evidence for boredom, confusion, interest, pride, and shame facial expressions, as well as contempt, relief, and triumph vocal expressions.[42][43][44]

Robert Plutchik agreed with Ekman's biologically driven perspective but developed the "wheel of emotions", suggesting eight primary emotions grouped on a positive or negative basis: joy versus sadness; anger versus fear; trust versus disgust; and surprise versus anticipation.[45] Some basic emotions can be modified to form complex emotions. The complex emotions could arise from cultural conditioning or association combined with the basic emotions. Alternatively, similar to the way primary colors combine, primary emotions could blend to form the full spectrum of human emotional experience. For example, interpersonal anger and disgust could blend to form contempt. Relationships exist between basic emotions, resulting in positive or negative influences.[46]

Multi-dimensional analysis
Sorting emotions into unpleasant-pleasant and activated-calm.
Two dimensions of emotions. Made accessible for practical use.[47]

Two dimensions of emotion
Psychologists have used methods such as factor analysis to attempt to map emotion-related responses onto a more limited number of dimensions. Such methods attempt to boil emotions down to underlying dimensions that capture the similarities and differences between experiences.[48] Often, the first two dimensions uncovered by factor analysis are valence (how negative or positive the experience feels) and arousal (how energized or enervated the experience feels). These two dimensions can be depicted on a 2D coordinate map.[49] This two-dimensional map has been theorized to capture one important component of emotion called core affect.[50][51] Core affect is not theorized to be the only component to emotion, but to give the emotion its hedonic and felt energy.

Using statistical methods to analyze emotional states elicited by short videos, Cowen and Keltner identified 27 varieties of emotional experience: admiration, adoration, aesthetic appreciation, amusement, anger, anxiety, awe, awkwardness, boredom, calmness, confusion, craving, disgust, empathic pain, entrancement, excitement, fear, horror, interest, joy, nostalgia, relief, romance, sadness, satisfaction, sexual desire and surprise.[52]

Theories
See also: Functional accounts of emotion
Pre-modern history
In Buddhism, emotions occur when an object is considered as attractive or repulsive. There is a felt tendency impelling people towards attractive objects and impelling them to move away from repulsive or harmful objects; a disposition to possess the object (greed), to destroy it (hatred), to flee from it (fear), to get obsessed or worried over it (anxiety), and so on.[53]

In Stoic theories, normal emotions (like delight and fear) are described as irrational impulses which come from incorrect appraisals of what is 'good' or 'bad'. Alternatively, there are 'good emotions' (like joy and caution) experienced by those that are wise, which come from correct appraisals of what is 'good' and 'bad'.[54][55]

Aristotle believed that emotions were an essential component of virtue.[56] In the Aristotelian view all emotions (called passions) corresponded to appetites or capacities. During the Middle Ages, the Aristotelian view was adopted and further developed by scholasticism and Thomas Aquinas[57] in particular.

In Chinese antiquity, excessive emotion was believed to cause damage to qi, which in turn, damages the vital organs.[58] The four humours theory made popular by Hippocrates contributed to the study of emotion in the same way that it did for medicine.

In the early 11th century, Avicenna theorized about the influence of emotions on health and behaviors, suggesting the need to manage emotions.[59]

Early modern views on emotion are developed in the works of philosophers such as René Descartes, Niccolò Machiavelli, Baruch Spinoza,[60] Thomas Hobbes[61] and David Hume. In the 19th century emotions were considered adaptive and were studied more frequently from an empiricist psychiatric perspective.

Western theological
Christian perspective on emotion presupposes a theistic origin to humanity. God who created humans gave humans the ability to feel emotion and interact emotionally. Biblical content expresses that God is a person who feels and expresses emotion. Though a somatic view would place the locus of emotions in the physical body, Christian theory of emotions would view the body more as a platform for the sensing and expression of emotions. Therefore emotions themselves arise from the person, or that which is "imago-dei" or image of God in humans. In Christian thought, emotions have the potential to be controlled through reasoned reflection. That reasoned reflection also mimics God who made mind. The purpose of emotions in human life are therefore summarized in God's call to enjoy Him and creation, humans are to enjoy emotions and benefit from them and use them to energize behavior.

Evolutionary theories
Main articles: Evolution of emotion and Evolutionary psychology

Illustration from Charles Darwin's The Expression of the Emotions in Man and Animals (1872)
19th century
Perspectives on emotions from evolutionary theory were initiated during the mid-late 19th century with Charles Darwin's 1872 book The Expression of the Emotions in Man and Animals.[62] Surprisingly, Darwin argued that emotions served no evolved purpose for humans, neither in communication, nor in aiding survival.[63] Darwin largely argued that emotions evolved via the inheritance of acquired characters.[64] He pioneered various methods for studying non-verbal expressions, from which he concluded that some expressions had cross-cultural universality. Darwin also detailed homologous expressions of emotions that occur in animals. This led the way for animal research on emotions and the eventual determination of the neural underpinnings of emotion.

Contemporary
More contemporary views along the evolutionary psychology spectrum posit that both basic emotions and social emotions evolved to motivate (social) behaviors that were adaptive in the ancestral environment.[13] Emotion is an essential part of any human decision-making and planning, and the famous distinction made between reason and emotion is not as clear as it seems.[65] Paul D. MacLean claims that emotion competes with even more instinctive responses, on one hand, and the more abstract reasoning, on the other hand. The increased potential in neuroimaging has also allowed investigation into evolutionarily ancient parts of the brain. Important neurological advances were derived from these perspectives in the 1990s by Joseph E. LeDoux and António Damásio.

Research on social emotion also focuses on the physical displays of emotion including body language of animals and humans (see affect display). For example, spite seems to work against the individual but it can establish an individual's reputation as someone to be feared.[13] Shame and pride can motivate behaviors that help one maintain one's standing in a community, and self-esteem is one's estimate of one's status.[13][66]

Somatic theories
Somatic theories of emotion claim that bodily responses, rather than cognitive interpretations, are essential to emotions. The first modern version of such theories came from William James in the 1880s. The theory lost favor in the 20th century, but has regained popularity more recently due largely to theorists such as John Cacioppo,[67] António Damásio,[68] Joseph E. LeDoux[69] and Robert Zajonc[70] who are able to appeal to neurological evidence.[71]

James–Lange theory
Main article: James–Lange theory

Simplified graph of James-Lange Theory of Emotion
In his 1884 article[72] William James argued that feelings and emotions were secondary to physiological phenomena. In his theory, James proposed that the perception of what he called an "exciting fact" directly led to a physiological response, known as "emotion."[73] To account for different types of emotional experiences, James proposed that stimuli trigger activity in the autonomic nervous system, which in turn produces an emotional experience in the brain. The Danish psychologist Carl Lange also proposed a similar theory at around the same time, and therefore this theory became known as the James–Lange theory. As James wrote, "the perception of bodily changes, as they occur, is the emotion." James further claims that "we feel sad because we cry, angry because we strike, afraid because we tremble, and either we cry, strike, or tremble because we are sorry, angry, or fearful, as the case may be."[72]

An example of this theory in action would be as follows: An emotion-evoking stimulus (snake) triggers a pattern of physiological response (increased heart rate, faster breathing, etc.), which is interpreted as a particular emotion (fear). This theory is supported by experiments in which by manipulating the bodily state induces a desired emotional state.[74] Some people may believe that emotions give rise to emotion-specific actions, for example, "I'm crying because I'm sad," or "I ran away because I was scared." The issue with the James–Lange theory is that of causation (bodily states causing emotions and being a priori), not that of the bodily influences on emotional experience (which can be argued and is still quite prevalent today in biofeedback studies and embodiment theory).[75]

Although mostly abandoned in its original form, Tim Dalgleish argues that most contemporary neuroscientists have embraced the components of the James-Lange theory of emotions.[76]

The James–Lange theory has remained influential. Its main contribution is the emphasis it places on the embodiment of emotions, especially the argument that changes in the bodily concomitants of emotions can alter their experienced intensity. Most contemporary neuroscientists would endorse a modified James–Lange view in which bodily feedback modulates the experience of emotion. (p. 583)

Cannon–Bard theory
Main article: Cannon–Bard theory
Walter Bradford Cannon agreed that physiological responses played a crucial role in emotions, but did not believe that physiological responses alone could explain subjective emotional experiences. He argued that physiological responses were too slow and often imperceptible and this could not account for the relatively rapid and intense subjective awareness of emotion.[77] He also believed that the richness, variety, and temporal course of emotional experiences could not stem from physiological reactions, that reflected fairly undifferentiated fight or flight responses.[78][79] An example of this theory in action is as follows: An emotion-evoking event (snake) triggers simultaneously both a physiological response and a conscious experience of an emotion.

Phillip Bard contributed to the theory with his work on animals. Bard found that sensory, motor, and physiological information all had to pass through the diencephalon (particularly the thalamus), before being subjected to any further processing. Therefore, Cannon also argued that it was not anatomically possible for sensory events to trigger a physiological response prior to triggering conscious awareness and emotional stimuli had to trigger both physiological and experiential aspects of emotion simultaneously.[78]

Two-factor theory
Main article: Two-factor theory of emotion
Stanley Schachter formulated his theory on the earlier work of a Spanish physician, Gregorio Marañón, who injected patients with epinephrine and subsequently asked them how they felt. Marañón found that most of these patients felt something but in the absence of an actual emotion-evoking stimulus, the patients were unable to interpret their physiological arousal as an experienced emotion. Schachter did agree that physiological reactions played a big role in emotions. He suggested that physiological reactions contributed to emotional experience by facilitating a focused cognitive appraisal of a given physiologically arousing event and that this appraisal was what defined the subjective emotional experience. Emotions were thus a result of two-stage process: general physiological arousal, and experience of emotion. For example, the physiological arousal, heart pounding, in a response to an evoking stimulus, the sight of a bear in the kitchen. The brain then quickly scans the area, to explain the pounding, and notices the bear. Consequently, the brain interprets the pounding heart as being the result of fearing the bear.[80] With his student, Jerome Singer, Schachter demonstrated that subjects can have different emotional reactions despite being placed into the same physiological state with an injection of epinephrine. Subjects were observed to express either anger or amusement depending on whether another person in the situation (a confederate) displayed that emotion. Hence, the combination of the appraisal of the situation (cognitive) and the participants' reception of adrenaline or a placebo together determined the response. This experiment has been criticized in Jesse Prinz's (2004) Gut Reactions.[81]

Cognitive theories
With the two-factor theory now incorporating cognition, several theories began to argue that cognitive activity in the form of judgments, evaluations, or thoughts were entirely necessary for an emotion to occur. One of the main proponents of this view was Richard Lazarus who argued that emotions must have some cognitive intentionality. The cognitive activity involved in the interpretation of an emotional context may be conscious or unconscious and may or may not take the form of conceptual processing.

Lazarus' theory is very influential; emotion is a disturbance that occurs in the following order:

Cognitive appraisal – The individual assesses the event cognitively, which cues the emotion.
Physiological changes – The cognitive reaction starts biological changes such as increased heart rate or pituitary adrenal response.
Action – The individual feels the emotion and chooses how to react.
For example: Jenny sees a snake.

Jenny cognitively assesses the snake in her presence. Cognition allows her to understand it as a danger.
Her brain activates the adrenal glands which pump adrenaline through her blood stream, resulting in increased heartbeat.
Jenny screams and runs away.
Lazarus stressed that the quality and intensity of emotions are controlled through cognitive processes. These processes underline coping strategies that form the emotional reaction by altering the relationship between the person and the environment.

George Mandler provided an extensive theoretical and empirical discussion of emotion as influenced by cognition, consciousness, and the autonomic nervous system in two books (Mind and Emotion, 1975,[82] and Mind and Body: Psychology of Emotion and Stress, 1984[83])

There are some theories on emotions arguing that cognitive activity in the form of judgments, evaluations, or thoughts are necessary in order for an emotion to occur. A prominent philosophical exponent is Robert C. Solomon (for example, The Passions, Emotions and the Meaning of Life, 1993[84]). Solomon claims that emotions are judgments. He has put forward a more nuanced view which responds to what he has called the 'standard objection' to cognitivism, the idea that a judgment that something is fearsome can occur with or without emotion, so judgment cannot be identified with emotion. The theory proposed by Nico Frijda where appraisal leads to action tendencies is another example.

It has also been suggested that emotions (affect heuristics, feelings and gut-feeling reactions) are often used as shortcuts to process information and influence behavior.[85] The affect infusion model (AIM) is a theoretical model developed by Joseph Forgas in the early 1990s that attempts to explain how emotion and mood interact with one's ability to process information.

Perceptual theory
Theories dealing with perception either use one or multiples perceptions in order to find an emotion.[86] A recent hybrid of the somatic and cognitive theories of emotion is the perceptual theory. This theory is neo-Jamesian in arguing that bodily responses are central to emotions, yet it emphasizes the meaningfulness of emotions or the idea that emotions are about something, as is recognized by cognitive theories. The novel claim of this theory is that conceptually-based cognition is unnecessary for such meaning. Rather the bodily changes themselves perceive the meaningful content of the emotion because of being causally triggered by certain situations. In this respect, emotions are held to be analogous to faculties such as vision or touch, which provide information about the relation between the subject and the world in various ways. A sophisticated defense of this view is found in philosopher Jesse Prinz's book Gut Reactions,[81] and psychologist James Laird's book Feelings.[74]

Affective events theory
Affective events theory is a communication-based theory developed by Howard M. Weiss and Russell Cropanzano (1996),[87] that looks at the causes, structures, and consequences of emotional experience (especially in work contexts). This theory suggests that emotions are influenced and caused by events which in turn influence attitudes and behaviors. This theoretical frame also emphasizes time in that human beings experience what they call emotion episodes –\ a "series of emotional states extended over time and organized around an underlying theme." This theory has been utilized by numerous researchers to better understand emotion from a communicative lens, and was reviewed further by Howard M. Weiss and Daniel J. Beal in their article, "Reflections on Affective Events Theory", published in Research on Emotion in Organizations in 2005.[88]

Situated perspective on emotion
A situated perspective on emotion, developed by Paul E. Griffiths and Andrea Scarantino, emphasizes the importance of external factors in the development and communication of emotion, drawing upon the situationism approach in psychology.[89] This theory is markedly different from both cognitivist and neo-Jamesian theories of emotion, both of which see emotion as a purely internal process, with the environment only acting as a stimulus to the emotion. In contrast, a situationist perspective on emotion views emotion as the product of an organism investigating its environment, and observing the responses of other organisms. Emotion stimulates the evolution of social relationships, acting as a signal to mediate the behavior of other organisms. In some contexts, the expression of emotion (both voluntary and involuntary) could be seen as strategic moves in the transactions between different organisms. The situated perspective on emotion states that conceptual thought is not an inherent part of emotion, since emotion is an action-oriented form of skillful engagement with the world. Griffiths and Scarantino suggested that this perspective on emotion could be helpful in understanding phobias, as well as the emotions of infants and animals.

Genetics
Emotions can motivate social interactions and relationships and therefore are directly related with basic physiology, particularly with the stress systems. This is important because emotions are related to the anti-stress complex, with an oxytocin-attachment system, which plays a major role in bonding. Emotional phenotype temperaments affect social connectedness and fitness in complex social systems.[90] These characteristics are shared with other species and taxa and are due to the effects of genes and their continuous transmission. Information that is encoded in the DNA sequences provides the blueprint for assembling proteins that make up our cells. Zygotes require genetic information from their parental germ cells, and at every speciation event, heritable traits that have enabled its ancestor to survive and reproduce successfully are passed down along with new traits that could be potentially beneficial to the offspring.

In the five million years since the lineages leading to modern humans and chimpanzees split, only about 1.2% of their genetic material has been modified. This suggests that everything that separates us from chimpanzees must be encoded in that very small amount of DNA, including our behaviors. Students that study animal behaviors have only identified intraspecific examples of gene-dependent behavioral phenotypes. In voles (Microtus spp.) minor genetic differences have been identified in a vasopressin receptor gene that corresponds to major species differences in social organization and the mating system.[91] Another potential example with behavioral differences is the FOCP2 gene, which is involved in neural circuitry handling speech and language.[92] Its present form in humans differed from that of the chimpanzees by only a few mutations and has been present for about 200,000 years, coinciding with the beginning of modern humans.[93] Speech, language, and social organization are all part of the basis for emotions.

Formation

Timeline of some of the most prominent brain models of emotion in affective neuroscience.
Neurobiological explanation
Based on discoveries made through neural mapping of the limbic system, the neurobiological explanation of human emotion is that emotion is a pleasant or unpleasant mental state organized in the limbic system of the mammalian brain. If distinguished from reactive responses of reptiles, emotions would then be mammalian elaborations of general vertebrate arousal patterns, in which neurochemicals (for example, dopamine, noradrenaline, and serotonin) step-up or step-down the brain's activity level, as visible in body movements, gestures and postures. Emotions can likely be mediated by pheromones (see fear).[35]

For example, the emotion of love is proposed to be the expression of Paleocircuits of the mammalian brain (specifically, modules of the cingulate gyrus) which facilitate the care, feeding, and grooming of offspring. Paleocircuits are neural platforms for bodily expression configured before the advent of cortical circuits for speech. They consist of pre-configured pathways or networks of nerve cells in the forebrain, brain stem and spinal cord.

Other emotions like fear and anxiety long thought to be exclusively generated by the most primitive parts of the brain (stem) and more associated to the fight-or-flight responses of behavior, have also been associated as adaptive expressions of defensive behavior whenever a threat is encountered. Although defensive behaviors have been present in a wide variety of species, Blanchard et al. (2001) discovered a correlation of given stimuli and situation that resulted in a similar pattern of defensive behavior towards a threat in human and non-human mammals.[94]

Whenever, potentially dangerous stimuli is presented additional brain structures activate that previously thought (hippocampus, thalamus, etc). Thus, giving the amygdala an important role on coordinating the following behavioral input based on the presented neurotransmitters that respond to threat stimuli. These biological functions of the amygdala are not only limited to the "fear-conditioning" and "processing of aversive stimuli", but also are present on other components of the amygdala. Therefore, it can referred the amygdala as a key structure to understand the potential responses of behavior in danger like situations in human and non-human mammals.[95]

The motor centers of reptiles react to sensory cues of vision, sound, touch, chemical, gravity, and motion with pre-set body movements and programmed postures. With the arrival of night-active mammals, smell replaced vision as the dominant sense, and a different way of responding arose from the olfactory sense, which is proposed to have developed into mammalian emotion and emotional memory. The mammalian brain invested heavily in olfaction to succeed at night as reptiles slept – one explanation for why olfactory lobes in mammalian brains are proportionally larger than in the reptiles. These odor pathways gradually formed the neural blueprint for what was later to become our limbic brain.[35]

Emotions are thought to be related to certain activities in brain areas that direct our attention, motivate our behavior, and determine the significance of what is going on around us. Pioneering work by Paul Broca (1878),[96] James Papez (1937),[97] and Paul D. MacLean (1952)[98] suggested that emotion is related to a group of structures in the center of the brain called the limbic system, which includes the hypothalamus, cingulate cortex, hippocampi, and other structures. More recent research has shown that some of these limbic structures are not as directly related to emotion as others are while some non-limbic structures have been found to be of greater emotional relevance.

Prefrontal cortex
There is ample evidence that the left prefrontal cortex is activated by stimuli that cause positive approach.[99] If attractive stimuli can selectively activate a region of the brain, then logically the converse should hold, that selective activation of that region of the brain should cause a stimulus to be judged more positively. This was demonstrated for moderately attractive visual stimuli[100] and replicated and extended to include negative stimuli.[101]

Two neurobiological models of emotion in the prefrontal cortex made opposing predictions. The valence model predicted that anger, a negative emotion, would activate the right prefrontal cortex. The direction model predicted that anger, an approach emotion, would activate the left prefrontal cortex. The second model was supported.[102]

This still left open the question of whether the opposite of approach in the prefrontal cortex is better described as moving away (direction model), as unmoving but with strength and resistance (movement model), or as unmoving with passive yielding (action tendency model). Support for the action tendency model (passivity related to right prefrontal activity) comes from research on shyness[103] and research on behavioral inhibition.[104] Research that tested the competing hypotheses generated by all four models also supported the action tendency model.[105][106]


Homeostatic/primordial emotion
Another neurological approach proposed by Bud Craig in 2003 distinguishes two classes of emotion: "classical" emotions such as love, anger and fear that are evoked by environmental stimuli, and "homeostatic emotions" – attention-demanding feelings evoked by body states, such as pain, hunger and fatigue, that motivate behavior (withdrawal, eating or resting in these examples) aimed at maintaining the body's internal milieu at its ideal state.[107]

Derek Denton calls the latter "primordial emotions" and defines them as "the subjective element of the instincts, which are the genetically programmed behavior patterns which contrive homeostasis. They include thirst, hunger for air, hunger for food, pain and hunger for specific minerals etc. There are two constituents of a primordial emotion--the specific sensation which when severe may be imperious, and the compelling intention for gratification by a consummatory act."[108]

Emergent explanation
Joseph LeDoux differentiates between the human's defense system, which has evolved over time, and emotions such as fear and anxiety. He has said that the amygdala may release hormones due to a trigger (such as an innate reaction to seeing a snake), but "then we elaborate it through cognitive and conscious processes".[33]

Lisa Feldman Barrett highlights differences in emotions between different cultures,[109] and says that emotions (such as anxiety) "are not triggered; you create them. They emerge as a combination of the physical properties of your body, a flexible brain that wires itself to whatever environment it develops in, and your culture and upbringing, which provide that environment."[110] She has termed this approach the theory of constructed emotion.

Disciplinary approaches
Many different disciplines have produced work on the emotions. Human sciences study the role of emotions in mental processes, disorders, and neural mechanisms. In psychiatry, emotions are examined as part of the discipline's study and treatment of mental disorders in humans. Nursing studies emotions as part of its approach to the provision of holistic health care to humans. Psychology examines emotions from a scientific perspective by treating them as mental processes and behavior and they explore the underlying physiological and neurological processes, e.g., cognitive-behavioral therapy. In neuroscience sub-fields such as social neuroscience and affective neuroscience, scientists study the neural mechanisms of emotion by combining neuroscience with the psychological study of personality, emotion, and mood. In linguistics, the expression of emotion may change to the meaning of sounds. In education, the role of emotions in relation to learning is examined.

Social sciences often examine emotion for the role that it plays in human culture and social interactions. In sociology, emotions are examined for the role they play in human society, social patterns and interactions, and culture. In anthropology, the study of humanity, scholars use ethnography to undertake contextual analyses and cross-cultural comparisons of a range of human activities. Some anthropology studies examine the role of emotions in human activities. In the field of communication sciences, critical organizational scholars have examined the role of emotions in organizations, from the perspectives of managers, employees, and even customers. A focus on emotions in organizations can be credited to Arlie Russell Hochschild's concept of emotional labor. The University of Queensland hosts EmoNet,[111] an e-mail distribution list representing a network of academics that facilitates scholarly discussion of all matters relating to the study of emotion in organizational settings. The list was established in January 1997 and has over 700 members from across the globe.

In economics, the social science that studies the production, distribution, and consumption of goods and services, emotions are analyzed in some sub-fields of microeconomics, in order to assess the role of emotions on purchase decision-making and risk perception. In criminology, a social science approach to the study of crime, scholars often draw on behavioral sciences, sociology, and psychology; emotions are examined in criminology issues such as anomie theory and studies of "toughness," aggressive behavior, and hooliganism. In law, which underpins civil obedience, politics, economics and society, evidence about people's emotions is often raised in tort law claims for compensation and in criminal law prosecutions against alleged lawbreakers (as evidence of the defendant's state of mind during trials, sentencing, and parole hearings). In political science, emotions are examined in a number of sub-fields, such as the analysis of voter decision-making.

In philosophy, emotions are studied in sub-fields such as ethics, the philosophy of art (for example, sensory–emotional values, and matters of taste and sentimentality), and the philosophy of music (see also music and emotion). In history, scholars examine documents and other sources to interpret and analyze past activities; speculation on the emotional state of the authors of historical documents is one of the tools of interpretation. In literature and film-making, the expression of emotion is the cornerstone of genres such as drama, melodrama, and romance. In communication studies, scholars study the role that emotion plays in the dissemination of ideas and messages. Emotion is also studied in non-human animals in ethology, a branch of zoology which focuses on the scientific study of animal behavior. Ethology is a combination of laboratory and field science, with strong ties to ecology and evolution. Ethologists often study one type of behavior (for example, aggression) in a number of unrelated animals.

History
The history of emotions has become an increasingly popular topic recently, with some scholars[who?] arguing that it is an essential category of analysis, not unlike class, race, or gender. Historians, like other social scientists, assume that emotions, feelings and their expressions are regulated in different ways by both different cultures and different historical times, and the constructivist school of history claims even that some sentiments and meta-emotions, for example schadenfreude, are learnt and not only regulated by culture. Historians of emotion trace and analyze the changing norms and rules of feeling, while examining emotional regimes, codes, and lexicons from social, cultural, or political history perspectives. Others focus on the history of medicine, science, or psychology. What somebody can and may feel (and show) in a given situation, towards certain people or things, depends on social norms and rules; thus historically variable and open to change.[112] Several research centers have opened in the past few years in Germany, England, Spain,[113] Sweden, and Australia.

Furthermore, research in historical trauma suggests that some traumatic emotions can be passed on from parents to offspring to second and even third generation, presented as examples of transgenerational trauma.

Sociology
Main article: Sociology of emotions
A common way in which emotions are conceptualized in sociology is in terms of the multidimensional characteristics including cultural or emotional labels (for example, anger, pride, fear, happiness), physiological changes (for example, increased perspiration, changes in pulse rate), expressive facial and body movements (for example, smiling, frowning, baring teeth), and appraisals of situational cues.[15] One comprehensive theory of emotional arousal in humans has been developed by Jonathan Turner (2007: 2009).[114][115] Two of the key eliciting factors for the arousal of emotions within this theory are expectations states and sanctions. When people enter a situation or encounter with certain expectations for how the encounter should unfold, they will experience different emotions depending on the extent to which expectations for Self, other and situation are met or not met. People can also provide positive or negative sanctions directed at Self or other which also trigger different emotional experiences in individuals. Turner analyzed a wide range of emotion theories across different fields of research including sociology, psychology, evolutionary science, and neuroscience. Based on this analysis, he identified four emotions that all researchers consider being founded on human neurology including assertive-anger, aversion-fear, satisfaction-happiness, and disappointment-sadness. These four categories are called primary emotions and there is some agreement amongst researchers that these primary emotions become combined to produce more elaborate and complex emotional experiences. These more elaborate emotions are called first-order elaborations in Turner's theory and they include sentiments such as pride, triumph, and awe. Emotions can also be experienced at different levels of intensity so that feelings of concern are a low-intensity variation of the primary emotion aversion-fear whereas depression is a higher intensity variant.

Attempts are frequently made to regulate emotion according to the conventions of the society and the situation based on many (sometimes conflicting) demands and expectations which originate from various entities. The expression of anger is in many cultures discouraged in girls and women to a greater extent than in boys and men (the notion being that an angry man has a valid complaint that needs to be rectified, while an angry women is hysterical or oversensitive, and her anger is somehow invalid), while the expression of sadness or fear is discouraged in boys and men relative to girls and women (attitudes implicit in phrases like "man up" or "don't be a sissy").[116][117] Expectations attached to social roles, such as "acting as man" and not as a woman, and the accompanying "feeling rules" contribute to the differences in expression of certain emotions. Some cultures encourage or discourage happiness, sadness, or jealousy, and the free expression of the emotion of disgust is considered socially unacceptable in most cultures. Some social institutions are seen as based on certain emotion, such as love in the case of contemporary institution of marriage. In advertising, such as health campaigns and political messages, emotional appeals are commonly found. Recent examples include no-smoking health campaigns and political campaigns emphasizing the fear of terrorism.[118]

Sociological attention to emotion has varied over time. Émile Durkheim (1915/1965)[119] wrote about the collective effervescence or emotional energy that was experienced by members of totemic rituals in Australian aborigine society. He explained how the heightened state of emotional energy achieved during totemic rituals transported individuals above themselves giving them the sense that they were in the presence of a higher power, a force, that was embedded in the sacred objects that were worshipped. These feelings of exaltation, he argued, ultimately lead people to believe that there were forces that governed sacred objects.

In the 1990s, sociologists focused on different aspects of specific emotions and how these emotions were socially relevant. For Cooley (1992),[120] pride and shame were the most important emotions that drive people to take various social actions. During every encounter, he proposed that we monitor ourselves through the "looking glass" that the gestures and reactions of others provide. Depending on these reactions, we either experience pride or shame and this results in particular paths of action. Retzinger (1991)[121] conducted studies of married couples who experienced cycles of rage and shame. Drawing predominantly on Goffman and Cooley's work, Scheff (1990)[122] developed a micro sociological theory of the social bond. The formation or disruption of social bonds is dependent on the emotions that people experience during interactions.

Subsequent to these developments, Randall Collins (2004)[123] formulated his interaction ritual theory by drawing on Durkheim's work on totemic rituals that was extended by Goffman (1964/2013; 1967)[124][125] into everyday focused encounters. Based on interaction ritual theory, we experience different levels or intensities of emotional energy during face-to-face interactions. Emotional energy is considered to be a feeling of confidence to take action and a boldness that one experiences when they are charged up from the collective effervescence generated during group gatherings that reach high levels of intensity.

There is a growing body of research applying the sociology of emotion to understanding the learning experiences of students during classroom interactions with teachers and other students (for example, Milne & Otieno, 2007;[126] Olitsky, 2007;[127] Tobin, et al., 2013;[128] Zembylas, 2002[129]). These studies show that learning subjects like science can be understood in terms of classroom interaction rituals that generate emotional energy and collective states of emotional arousal like emotional climate.

Apart from interaction ritual traditions of the sociology of emotion, other approaches have been classed into one of six other categories:[115]

evolutionary/biological theories
symbolic interactionist theories
dramaturgical theories
ritual theories
power and status theories
stratification theories
exchange theories
This list provides a general overview of different traditions in the sociology of emotion that sometimes conceptualise emotion in different ways and at other times in complementary ways. Many of these different approaches were synthesized by Turner (2007) in his sociological theory of human emotions in an attempt to produce one comprehensive sociological account that draws on developments from many of the above traditions.[114]

[92] [93] [91]

Psychotherapy and regulation
Emotion regulation refers to the cognitive and behavioral strategies people use to influence their own emotional experience.[130] For example, a behavioral strategy in which one avoids a situation to avoid unwanted emotions (trying not to think about the situation, doing distracting activities, etc.).[131] Depending on the particular school's general emphasis on either cognitive components of emotion, physical energy discharging, or on symbolic movement and facial expression components of emotion different schools of psychotherapy approach the regulation of emotion differently. Cognitively oriented schools approach them via their cognitive components, such as rational emotive behavior therapy. Yet others approach emotions via symbolic movement and facial expression components (like in contemporary Gestalt therapy).[132]

Cross-cultural research
Research on emotions reveals the strong presence of cross-cultural differences in emotional reactions and that emotional reactions are likely to be culture-specific.[133] In strategic settings, cross-cultural research on emotions is required for understanding the psychological situation of a given population or specific actors. This implies the need to comprehend the current emotional state, mental disposition or other behavioral motivation of a target audience located in a different culture, basically founded on its national political, social, economic, and psychological peculiarities but also subject to the influence of circumstances and events.[134]

Computer science
Main article: Affective computing
In the 2000s, research in computer science, engineering, psychology and neuroscience has been aimed at developing devices that recognize human affect display and model emotions.[135] In computer science, affective computing is a branch of the study and development of artificial intelligence that deals with the design of systems and devices that can recognize, interpret, and process human emotions. It is an interdisciplinary field spanning computer sciences, psychology, and cognitive science.[136] While the origins of the field may be traced as far back as to early philosophical enquiries into emotion,[72] the more modern branch of computer science originated with Rosalind Picard's 1995 paper[137] on affective computing.[138][139] Detecting emotional information begins with passive sensors which capture data about the user's physical state or behavior without interpreting the input. The data gathered is analogous to the cues humans use to perceive emotions in others. Another area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. Emotional speech processing recognizes the user's emotional state by analyzing speech patterns. The detection and processing of facial expression or body gestures is achieved through detectors and sensors.

The effects on memory
Emotion affects the way autobiographical memories are encoded and retrieved. Emotional memories are reactivated more, they are remembered better and have more attention devoted to them.[140] Through remembering our past achievements and failures, autobiographical memories affect how we perceive and feel about ourselves.[140]

Notable theorists

William James
In the late 19th century, the most influential theorists were William James (1842–1910) and Carl Lange (1834–1900). James was an American psychologist and philosopher who wrote about educational psychology, psychology of religious experience/mysticism, and the philosophy of pragmatism. Lange was a Danish physician and psychologist. Working independently, they developed the James–Lange theory, a hypothesis on the origin and nature of emotions. The theory states that within human beings, as a response to experiences in the world, the autonomic nervous system creates physiological events such as muscular tension, a rise in heart rate, perspiration, and dryness of the mouth. Emotions, then, are feelings which come about as a result of these physiological changes, rather than being their cause.[141]

Silvan Tomkins (1911–1991) developed the affect theory and script theory. The affect theory introduced the concept of basic emotions, and was based on the idea that the dominance of the emotion, which he called the affected system, was the motivating force in human life.[142]

Some of the most influential deceased theorists on emotion from the 20th century include Magda B. Arnold (1903–2002), an American psychologist who developed the appraisal theory of emotions;[143] Richard Lazarus (1922–2002), an American psychologist who specialized in emotion and stress, especially in relation to cognition; Herbert A. Simon (1916–2001), who included emotions into decision making and artificial intelligence; Robert Plutchik (1928–2006), an American psychologist who developed a psychoevolutionary theory of emotion;[144] Robert Zajonc (1923–2008) a Polish–American social psychologist who specialized in social and cognitive processes such as social facilitation; Robert C. Solomon (1942–2007), an American philosopher who contributed to the theories on the philosophy of emotions with books such as What Is An Emotion?: Classic and Contemporary Readings (2003);[145] Peter Goldie (1946–2011), a British philosopher who specialized in ethics, aesthetics, emotion, mood and character; Nico Frijda (1927–2015), a Dutch psychologist who advanced the theory that human emotions serve to promote a tendency to undertake actions that are appropriate in the circumstances, detailed in his book The Emotions (1986);[146] Jaak Panksepp (1943-2017), an Estonian-born American psychologist, psychobiologist, neuroscientist and pioneer in affective neuroscience.

Influential theorists who are still active include the following psychologists, neurologists, philosophers, and sociologists:
Logic (from Greek: λογική, logikḗ, 'possessed of reason, intellectual, dialectical, argumentative')[1][2][i] is the systematic study of valid rules of inference, i.e. the relations that lead to the acceptance of one proposition (the conclusion) on the basis of a set of other propositions (premises). More broadly, logic is the analysis and appraisal of arguments.[3]

There is no universal agreement as to the exact definition and boundaries of logic, hence the issue still remains one of the main subjects of research and debates in the field of philosophy of logic (see § Rival conceptions).[4][5][6] However, it has traditionally included the classification of arguments; the systematic exposition of the logical forms; the validity and soundness of deductive reasoning; the strength of inductive reasoning; the study of formal proofs and inference (including paradoxes and fallacies); and the study of syntax and semantics.

A good argument not only possesses validity and soundness (or strength, in induction), but it also avoids circular dependencies, is clearly stated, relevant, and consistent; otherwise it is useless for reasoning and persuasion, and is classified as a fallacy.[7]

In ordinary discourse, inferences may be signified by words such as therefore, thus, hence, ergo, and so on.

Historically, logic has been studied in philosophy (since ancient times) and mathematics (since the mid-19th century). More recently, logic has been studied in cognitive science, which draws on computer science, linguistics, philosophy and psychology, among other disciplines. A logician is any person, often a philosopher or mathematician, whose topic of scholarly study is logic.

Upon this first, and in one sense this sole, rule of reason, that in order to learn you must desire to learn, and in so desiring not be satisfied with what you already incline to capably think, there follows one corollary which itself deserves to be inscribed upon every wall of the city of philosophy: Do not block the way of inquiry.
Charles Sanders Peirce, First Rule of Logic
Philosophical logic
Philosophical logic is an area of philosophy. It's a set of methods used to solve philosophical problems and a fundamental tool for the advancement of metaphilosophy.

Informal logic
Informal logic is the study of natural language arguments. The study of fallacies is an important branch of informal logic. Since much informal argument is not strictly speaking deductive, on some conceptions of logic, informal logic is not logic at all. (See § Rival conceptions.)

Formal logic
Formal logic is the study of inference with purely formal content. An inference possesses a purely formal and explicit content (i.e. it can be expressed as a particular application of a wholly abstract rule) such as, a rule that is not about any particular thing or property. In many definitions of logic, logical consequence and inference with purely formal content are the same.

Examples of formal logic include (1) traditional syllogistic logic (a.k.a. term logic) and (2) modern symbolic Logic:

Syllogistic logic can be found in the works of Aristotle, making it the earliest known formal study and studies types of syllogism. Modern formal logic follows and expands on Aristotle.[8][9]
Symbolic logic is the study of symbolic abstractions that capture the formal features of logical inference,[10][11] often divided into two main branches: propositional logic and predicate logic.
Mathematical logic
Mathematical logic is an extension of symbolic logic into other areas, in particular to the study of model theory, proof theory, set theory, and computability theory.[12][13]

Concepts

Argument terminology used in logic
The concepts of logical form and argument are central to logic.

An argument is constructed by applying one of the forms of the different types of logical reasoning: deductive, inductive, and abductive. In deduction, the validity of an argument is determined solely by its logical form, not its content, whereas the soundness requires both validity and that all the given premises are actually true.[14]

Completeness, consistency, decidability, and expressivity, are further fundamental concepts in logic. The categorization of the logical systems and of their properties has led to the emergence of a metatheory of logic known as metalogic.[15] However, agreement on what logic actually is has remained elusive, although the field of universal logic has studied the common structure of logics.

Logical form
Main article: Logical form
Logic is generally considered formal when it analyzes and represents the form of any valid argument type. The form of an argument is displayed by representing its sentences in the formal grammar and symbolism of a logical language to make its content usable in formal inference. Simply put, to formalize simply means to translate English sentences into the language of logic.

This is called showing the logical form of the argument. It is necessary because indicative sentences of ordinary language show a considerable variety of form and complexity that makes their use in inference impractical. It requires, first, ignoring those grammatical features irrelevant to logic (such as gender and declension, if the argument is in Latin), replacing conjunctions irrelevant to logic (e.g. "but") with logical conjunctions like "and" and replacing ambiguous, or alternative logical expressions ("any", "every", etc.) with expressions of a standard type (e.g. "all", or the universal quantifier ∀).

Second, certain parts of the sentence must be replaced with schematic letters. Thus, for example, the expression "all Ps are Qs" shows the logical form common to the sentences "all men are mortals", "all cats are carnivores", "all Greeks are philosophers", and so on. The schema can further be condensed into the formula A(P,Q), where the letter A indicates the judgement 'all – are –'.

The importance of form was recognised from ancient times. Aristotle uses variable letters to represent valid inferences in Prior Analytics, leading Jan Łukasiewicz to say that the introduction of variables was "one of Aristotle's greatest inventions".[16] According to the followers of Aristotle (such as Ammonius), only the logical principles stated in schematic terms belong to logic, not those given in concrete terms. The concrete terms 'man', 'mortal', etc., are analogous to the substitution values of the schematic placeholders P, Q, R, which were called the 'matter' (Greek: ὕλη, hyle) of the inference.

There is a big difference between the kinds of formulas seen in traditional term logic and the predicate calculus that is the fundamental advance of modern logic. The formula A(P,Q) (all Ps are Qs) of traditional logic corresponds to the more complex formula {\displaystyle \forall x(P(x)\rightarrow Q(x))}{\displaystyle \forall x(P(x)\rightarrow Q(x))} in predicate logic, involving the logical connectives for universal quantification and implication rather than just the predicate letter A and using variable arguments {\displaystyle P(x)}P(x) where traditional logic uses just the term letter P. With the complexity comes power, and the advent of the predicate calculus inaugurated revolutionary growth of the subject.[citation needed][17]

Semantics
Main article: Semantics of logic
The validity of an argument depends upon the meaning, or semantics, of the sentences that make it up.

Aristotle's six Organon, especially De Interpretatione, gives a cursory outline of semantics which the scholastic logicians, particularly in the thirteenth and fourteenth century, developed into a complex and sophisticated theory, called supposition theory. This showed how the truth of simple sentences, expressed schematically, depend on how the terms 'supposit', or stand for, certain extra-linguistic items. For example, in part II of his Summa Logicae, William of Ockham presents a comprehensive account of the necessary and sufficient conditions for the truth of simple sentences, in order to show which arguments are valid and which are not. Thus "every A is B' is true if and only if there is something for which 'A' stands, and there is nothing for which 'A' stands, for which 'B' does not also stand."[18]

Early modern logic defined semantics purely as a relation between ideas. Antoine Arnauld in the Port Royal-Logic,[19][20] says that after conceiving things by our ideas, we compare these ideas, and, finding that some belong together and some do not, we unite or separate them. This is called affirming or denying, and in general judging.[21] Thus truth and falsity are no more than the agreement or disagreement of ideas. This suggests obvious difficulties, leading Locke to distinguish between 'real' truth, when our ideas have 'real existence' and 'imaginary' or 'verbal' truth, where ideas like harpies or centaurs exist only in the mind.[22] This view, known as psychologism, was taken to the extreme in the nineteenth century, and is generally held by modern logicians to signify a low point in the decline of logic before the twentieth century.

Modern semantics is in some ways closer to the medieval view, in rejecting such psychological truth-conditions. However, the introduction of quantification, needed to solve the problem of multiple generality, rendered impossible the kind of subject-predicate analysis that underlies medieval semantics. The main modern approach is model-theoretic semantics, based on Alfred Tarski's semantic theory of truth. The approach assumes that the meaning of the various parts of the propositions are given by the possible ways we can give a recursively specified group of interpretation functions from them to some predefined domain of discourse: an interpretation of first-order predicate logic is given by a mapping from terms to a universe of individuals, and a mapping from propositions to the truth values "true" and "false". Model-theoretic semantics is one of the fundamental concepts of model theory. Modern semantics also admits rival approaches, such as the proof-theoretic semantics that associates the meaning of propositions with the roles that they can play in inferences, an approach that ultimately derives from the work of Gerhard Gentzen on structural proof theory and is heavily influenced by Ludwig Wittgenstein's later philosophy, especially his aphorism "meaning is use."

Inference
Inference is not to be confused with implication. An implication is a sentence of the form 'If p then q', and can be true or false. The stoic logician Philo of Megara was the first to define the truth conditions of such an implication: false only when the antecedent p is true and the consequent q is false, in all other cases true. An inference, on the other hand, consists of two separately asserted propositions of the form 'p therefore q'. An inference is not true or false, but valid or invalid. However, there is a connection between implication and inference, as follows: if the implication 'if p then q' is true, the inference 'p therefore q' is valid. This was given an apparently paradoxical formulation by Philo, who said that the implication 'if it is day, it is night' is true only at night, so the inference 'it is day, therefore it is night' is valid in the night, but not in the day.

The theory of inference (or 'consequences') was systematically developed in medieval times by logicians such as William of Ockham and Walter Burley. It is uniquely medieval, though it has its origins in Aristotle's Topica and Boethius' De Syllogismis hypotheticis. Many terms in logic, for this reason, are in Latin. For instance, the rule that licenses the move from the implication 'if p then q' plus the assertion of its antecedent p, to the assertion of the consequent q, is known as modus ponens ('mode of positing')—from Latin: posito antecedente ponitur consequens. The Latin formulations of many other rules such as ex falso quodlibet ('from falsehood, anything [follows]'), and reductio ad absurdum ('reduction to absurdity'; i.e. to disprove by showing the consequence as absurd), also date from this period.

However, the theory of consequences, or the so-called hypothetical syllogism, was never fully integrated into the theory of the categorical syllogism. This was partly because of the resistance to reducing the categorical judgment 'every s is p' to the so-called hypothetical judgment 'if anything is s, it is p'. The first was thought to imply 'some s is p', the latter was not, and as late as 1911 in the Encyclopædia Britannica article on "Logic", we find the Oxford logician T. H. Case arguing against Sigwart's and Brentano's modern analysis of the universal proposition.

Logical systems
Main article: Formal system
A formal system is an organization of terms used for the analysis of deduction. It consists of an alphabet, a language over the alphabet to construct sentences, and a rule for deriving sentences. Among the important properties that logical systems can have are:

Consistency: no theorem of the system contradicts another.[23]
Validity: the system's rules of proof never allow a false inference from true premises.
Completeness: if a formula is true, it can be proven, i.e. is a theorem of the system.
Soundness: if any formula is a theorem of the system, it is true. This is the converse of completeness. (Note that in a distinct philosophical use of the term, an argument is sound when it is both valid and its premises are true.)[14]
Expressivity: what concepts can be expressed in the system.
Some logical systems do not have all these properties. As an example, Kurt Gödel's incompleteness theorems show that sufficiently complex formal systems of arithmetic cannot be consistent and complete;[11] however, first-order predicate logics not extended by specific axioms to be arithmetic formal systems with equality can be complete and consistent.[24]

Logic and rationality
Main article: Logic and rationality

This section may be confusing or unclear to readers. Please help clarify the section. There might be a discussion about this on the talk page. (May 2016) (Learn how and when to remove this template message)
As the study of argument is of clear importance to the reasons that we hold things to be true, logic is of essential importance to rationality. Here we have defined logic to be "the systematic study of the form of arguments;" the reasoning behind argument is of several sorts, but only some of these arguments fall under the aegis of logic proper.

Deductive reasoning concerns the logical consequence of given premises and is the form of reasoning most closely connected to logic. On a narrow conception of logic (see below) logic concerns just deductive reasoning, although such a narrow conception controversially excludes most of what is called informal logic from the discipline.

There are other forms of reasoning that are rational but that are generally not taken to be part of logic. These include inductive reasoning, which covers forms of inference that move from collections of particular judgements to universal judgements, and abductive reasoning,[ii] which is a form of inference that goes from observation to a hypothesis that accounts for the reliable data (observation) and seeks to explain relevant evidence. American philosopher Charles Sanders Peirce (1839–1914) first introduced the term as guessing.[25] Peirce said that to abduce a hypothetical explanation {\displaystyle a}a from an observed surprising circumstance {\displaystyle b}b is to surmise that {\displaystyle a}a may be true because then {\displaystyle b}b would be a matter of course.[26] Thus, to abduce {\displaystyle a}a from {\displaystyle b}b involves determining that {\displaystyle a}a is sufficient (or nearly sufficient), but not necessary, for {\displaystyle b}b.[27][28][29]

While inductive and abductive inference are not part of logic proper, the methodology of logic has been applied to them with some degree of success. For example, the notion of deductive validity (where an inference is deductively valid if and only if there is no possible situation in which all the premises are true but the conclusion false) exists in an analogy to the notion of inductive validity, or "strength", where an inference is inductively strong if and only if its premises give some degree of probability to its conclusion. Whereas the notion of deductive validity can be rigorously stated for systems of formal logic in terms of the well-understood notions of semantics, inductive validity requires us to define a reliable generalization of some set of observations. The task of providing this definition may be approached in various ways, some less formal than others; some of these definitions may use logical association rule induction, while others may use mathematical models of probability such as decision trees.

Rival conceptions
Main article: Conceptions of logic
Logic arose (see below) from a concern with correctness of argumentation. Modern logicians usually wish to ensure that logic studies just those arguments that arise from appropriately general forms of inference. For example, Thomas Hofweber writes in the Stanford Encyclopedia of Philosophy that logic "does not, however, cover good reasoning as a whole. That is the job of the theory of rationality. Rather it deals with inferences whose validity can be traced back to the formal features of the representations that are involved in that inference, be they linguistic, mental, or other representations."[30]

The idea that logic treats special forms of argument, deductive argument, rather than argument in general, has a history in logic that dates back at least to logicism in mathematics (19th and 20th centuries) and the advent of the influence of mathematical logic on philosophy. A consequence of taking logic to treat special kinds of argument is that it leads to identification of special kinds of truth, the logical truths (with logic equivalently being the study of logical truth), and excludes many of the original objects of study of logic that are treated as informal logic. Robert Brandom has argued against the idea that logic is the study of a special kind of logical truth, arguing that instead one can talk of the logic of material inference (in the terminology of Wilfred Sellars), with logic making explicit the commitments that were originally implicit in informal inference.[31][page needed]

History
Main article: History of logic

Aristotle, 384–322 BCE.
Logic comes from the Greek word logos, originally meaning "the word" or "what is spoken", but coming to mean "thought" or "reason". In the Western World, logic was first developed by Aristotle, who called the subject 'analytics'.[32] Aristotelian logic became widely accepted in science and mathematics and remained in wide use in the West until the early 19th century.[33] Aristotle's system of logic was responsible for the introduction of hypothetical syllogism,[34] temporal modal logic,[35][36] and inductive logic,[37] as well as influential vocabulary such as terms, predicables, syllogisms and propositions. There was also the rival Stoic logic.

In Europe during the later medieval period, major efforts were made to show that Aristotle's ideas were compatible with Christian faith. During the High Middle Ages, logic became a main focus of philosophers, who would engage in critical logical analyses of philosophical arguments, often using variations of the methodology of scholasticism. In 1323, William of Ockham's influential Summa Logicae was released. By the 18th century, the structured approach to arguments had degenerated and fallen out of favour, as depicted in Holberg's satirical play Erasmus Montanus. The Chinese logical philosopher Gongsun Long (c. 325–250 BCE) proposed the paradox "One and one cannot become two, since neither becomes two."[13][iii] In China, the tradition of scholarly investigation into logic, however, was repressed by the Qin dynasty following the legalist philosophy of Han Feizi.

In India, the Anviksiki school of logic was founded by Medhātithi (c. 6th century BCE).[38] Innovations in the scholastic school, called Nyaya, continued from ancient times into the early 18th century with the Navya-Nyāya school. By the 16th century, it developed theories resembling modern logic, such as Gottlob Frege's "distinction between sense and reference of proper names" and his "definition of number", as well as the theory of "restrictive conditions for universals" anticipating some of the developments in modern set theory.[iv] Since 1824, Indian logic attracted the attention of many Western scholars, and has had an influence on important 19th-century logicians such as Charles Babbage, Augustus De Morgan, and George Boole.[39] In the 20th century, Western philosophers like Stanislaw Schayer and Klaus Glashoff have explored Indian logic more extensively.

The syllogistic logic developed by Aristotle predominated in the West until the mid-19th century, when interest in the foundations of mathematics stimulated the development of symbolic logic (now called mathematical logic). In 1854, George Boole published The Laws of Thought,[40] introducing symbolic logic and the principles of what is now known as Boolean logic. In 1879, Gottlob Frege published Begriffsschrift, which inaugurated modern logic with the invention of quantifier notation, reconciling the Aristotelian and Stoic logics in a broader system, and solving such problems for which Aristotelian logic was impotent, such as the problem of multiple generality. From 1910 to 1913, Alfred North Whitehead and Bertrand Russell published Principia Mathematica[10] on the foundations of mathematics, attempting to derive mathematical truths from axioms and inference rules in symbolic logic. In 1931, Gödel raised serious problems with the foundationalist program and logic ceased to focus on such issues.

The development of logic since Frege, Russell, and Wittgenstein had a profound influence on the practice of philosophy and the perceived nature of philosophical problems (see analytic philosophy) and philosophy of mathematics. Logic, especially sentential logic, is implemented in computer logic circuits and is fundamental to computer science. Logic is commonly taught by university philosophy, sociology, advertising and literature departments, often as a compulsory discipline.

Types
Syllogistic logic
Main article: Aristotelian logic

A depiction from the 15th century of the square of opposition, which expresses the fundamental dualities of syllogistic.
The Organon was Aristotle's body of work on logic, with the Prior Analytics constituting the first explicit work in formal logic, introducing the syllogistic.[16] The parts of syllogistic logic, also known by the name term logic, are the analysis of the judgements into propositions consisting of two terms that are related by one of a fixed number of relations, and the expression of inferences by means of syllogisms that consist of two propositions sharing a common term as premise, and a conclusion that is a proposition involving the two unrelated terms from the premises.

Aristotle's work was regarded in classical times and from medieval times in Europe and the Middle East as the very picture of a fully worked out system. However, it was not alone: the Stoics proposed a system of propositional logic that was studied by medieval logicians. Also, the problem of multiple generality was recognized in medieval times. Nonetheless, problems with syllogistic logic were not seen as being in need of revolutionary solutions.

Today, some academics claim that Aristotle's system is generally seen as having little more than historical value (though there is some current interest in extending term logics), regarded as made obsolete by the advent of propositional logic and the predicate calculus. Others use Aristotle in argumentation theory to help develop and critically question argumentation schemes that are used in artificial intelligence and legal arguments.

Propositional logic
Main article: Propositional calculus
A propositional calculus or logic (also a sentential calculus) is a formal system in which formulae representing propositions can be formed by combining atomic propositions (usually represented with p, q, etc.) using logical connectives ({\displaystyle \And ,\rightarrow ,\lor ,\equiv ,\sim ,}{\displaystyle \And ,\rightarrow ,\lor ,\equiv ,\sim ,} etc.); these propositions and connectives are the only elements of a standard propositional calculus.[41] Unlike predicate logic or syllogistic logic where individual subjects and predicates (which do not have truth values) are the smallest unit, propositional logic takes full propositions with truth values as its most basic component[42]. Quantifiers (e.g. {\displaystyle \forall }\forall  or {\displaystyle \exists }\exists ) are included in extended propositional calculus, but they only quantify over full propositions, not individual subjects or predicates[43]. A given propositional logic is a system of formal proof with rules that establish which well-formed formulae of a given language are "theorems" by proving them from axioms which are assumed without proof.[44]

Predicate logic

Gottlob Frege's Begriffschrift introduced the notion of quantifier in a graphical notation, which here represents the judgement that {\displaystyle \forall x.F(x)}{\displaystyle \forall x.F(x)} is true.
Main article: Predicate logic
Predicate logic is the generic term for symbolic formal systems such as first-order logic, second-order logic, many-sorted logic, and infinitary logic. It provides an account of quantifiers general enough to express a wide set of arguments occurring in natural language. For example, Bertrand Russell's famous barber paradox, "there is a man who shaves all and only men who do not shave themselves" can be formalised by the sentence {\displaystyle (\exists x)({\text{man}}(x)\wedge (\forall y)({\text{man}}(y)\rightarrow ({\text{shaves}}(x,y)\leftrightarrow \neg {\text{shaves}}(y,y))))}(\exists x)({\text{man}}(x)\wedge (\forall y)({\text{man}}(y)\rightarrow ({\text{shaves}}(x,y)\leftrightarrow \neg {\text{shaves}}(y,y)))), using the non-logical predicate {\displaystyle {\text{man}}(x)}{\displaystyle {\text{man}}(x)} to indicate that x is a man, and the non-logical relation {\displaystyle {\text{shaves}}(x,y)}{\displaystyle {\text{shaves}}(x,y)} to indicate that x shaves y; all other symbols of the formulae are logical, expressing the universal and existential quantifiers, conjunction, implication, negation and biconditional.

Whilst Aristotelian syllogistic logic specifies a small number of forms that the relevant part of the involved judgements may take, predicate logic allows sentences to be analysed into subject and argument in several additional ways—allowing predicate logic to solve the problem of multiple generality that had perplexed medieval logicians.

The development of predicate logic is usually attributed to Gottlob Frege, who is also credited as one of the founders of analytic philosophy, but the formulation of predicate logic most often used today is the first-order logic presented in Principles of Mathematical Logic by David Hilbert and Wilhelm Ackermann in 1928. The analytical generality of predicate logic allowed the formalization of mathematics, drove the investigation of set theory, and allowed the development of Alfred Tarski's approach to model theory. It provides the foundation of modern mathematical logic.

Frege's original system of predicate logic was second-order, rather than first-order. Second-order logic is most prominently defended (against the criticism of Willard Van Orman Quine and others) by George Boolos and Stewart Shapiro.

Modal logic
Main article: Modal logic
In languages, modality deals with the phenomenon that sub-parts of a sentence may have their semantics modified by special verbs or modal particles. For example, "We go to the games" can be modified to give "We should go to the games", and "We can go to the games" and perhaps "We will go to the games". More abstractly, we might say that modality affects the circumstances in which we take an assertion to be satisfied. Confusing modality is known as the modal fallacy.

Aristotle's logic is in large parts concerned with the theory of non-modalized logic. Although, there are passages in his work, such as the famous sea-battle argument in De Interpretatione § 9, that are now seen as anticipations of modal logic and its connection with potentiality and time, the earliest formal system of modal logic was developed by Avicenna, who ultimately developed a theory of "temporally modalized" syllogistic.[45]

While the study of necessity and possibility remained important to philosophers, little logical innovation happened until the landmark investigations of C. I. Lewis in 1918, who formulated a family of rival axiomatizations of the alethic modalities. His work unleashed a torrent of new work on the topic, expanding the kinds of modality treated to include deontic logic and epistemic logic. The seminal work of Arthur Prior applied the same formal language to treat temporal logic and paved the way for the marriage of the two subjects. Saul Kripke discovered (contemporaneously with rivals) his theory of frame semantics, which revolutionized the formal technology available to modal logicians and gave a new graph-theoretic way of looking at modality that has driven many applications in computational linguistics and computer science, such as dynamic logic.

Informal reasoning and dialectic
Main articles: Informal logic, Dialogical logic, and Logic and dialectic
The motivation for the study of logic in ancient times was clear: it is so that one may learn to distinguish good arguments from bad arguments, and so become more effective in argument and oratory, and perhaps also to become a better person. Half of the works of Aristotle's Organon treat inference as it occurs in an informal setting, side by side with the development of the syllogistic, and in the Aristotelian school, these informal works on logic were seen as complementary to Aristotle's treatment of rhetoric.

This ancient motivation is still alive, although it no longer takes centre stage in the picture of logic; typically dialectical logic forms the heart of a course in critical thinking, a compulsory course at many universities. Dialectic has been linked to logic since ancient times, but it has not been until recent decades that European and American logicians have attempted to provide mathematical foundations for logic and dialectic by formalising dialectical logic. Dialectical logic is also the name given to the special treatment of dialectic in Hegelian and Marxist thought. There have been pre-formal treatises on argument and dialectic, from authors such as Stephen Toulmin (The Uses of Argument), Nicholas Rescher (Dialectics),[46][47][48] and van Eemeren and Grootendorst (Pragma-dialectics). Theories of defeasible reasoning can provide a foundation for the formalisation of dialectical logic and dialectic itself can be formalised as moves in a game, where an advocate for the truth of a proposition and an opponent argue. Such games can provide a formal game semantics for many logics.

Argumentation theory is the study and research of informal logic, fallacies, and critical questions as they relate to every day and practical situations. Specific types of dialogue can be analyzed and questioned to reveal premises, conclusions, and fallacies. Argumentation theory is now applied in artificial intelligence and law.

Mathematical logic
Main article: Mathematical logic
Mathematical logic comprises two distinct areas of research: the first is the application of the techniques of formal logic to mathematics and mathematical reasoning, and the second, in the other direction, the application of mathematical techniques to the representation and analysis of formal logic.[49]

The earliest use of mathematics and geometry in relation to logic and philosophy goes back to the ancient Greeks such as Euclid, Plato, and Aristotle.[50] Many other ancient and medieval philosophers applied mathematical ideas and methods to their philosophical claims.[51]

One of the boldest attempts to apply logic to mathematics was the logicism pioneered by philosopher-logicians such as Gottlob Frege and Bertrand Russell. Mathematical theories were supposed to be logical tautologies, and the programme was to show this by means of a reduction of mathematics to logic.[10] The various attempts to carry this out met with failure, from the crippling of Frege's project in his Grundgesetze by Russell's paradox, to the defeat of Hilbert's program by Gödel's incompleteness theorems.

Both the statement of Hilbert's program and its refutation by Gödel depended upon their work establishing the second area of mathematical logic, the application of mathematics to logic in the form of proof theory.[52] Despite the negative nature of the incompleteness theorems, Gödel's completeness theorem, a result in model theory and another application of mathematics to logic, can be understood as showing how close logicism came to being true: every rigorously defined mathematical theory can be exactly captured by a first-order logical theory; Frege's proof calculus is enough to describe the whole of mathematics, though not equivalent to it.

If proof theory and model theory have been the foundation of mathematical logic, they have been but two of the four pillars of the subject.[53] Set theory originated in the study of the infinite by Georg Cantor, and it has been the source of many of the most challenging and important issues in mathematical logic, from Cantor's theorem, through the status of the Axiom of Choice and the question of the independence of the continuum hypothesis, to the modern debate on large cardinal axioms.

Recursion theory captures the idea of computation in logical and arithmetic terms; its most classical achievements are the undecidability of the Entscheidungsproblem by Alan Turing, and his presentation of the Church–Turing thesis.[54] Today recursion theory is mostly concerned with the more refined problem of complexity classes—when is a problem efficiently solvable?—and the classification of degrees of unsolvability.[55]

Philosophical logic
Main article: Philosophical logic
Philosophical logic deals with formal descriptions of ordinary, non-specialist ("natural") language, that is strictly only about the arguments within philosophy's other branches. Most philosophers assume that the bulk of everyday reasoning can be captured in logic if a method or methods to translate ordinary language into that logic can be found. Philosophical logic is essentially a continuation of the traditional discipline called "logic" before the invention of mathematical logic. Philosophical logic has a much greater concern with the connection between natural language and logic. As a result, philosophical logicians have contributed a great deal to the development of non-standard logics (e.g. free logics, tense logics) as well as various extensions of classical logic (e.g. modal logics) and non-standard semantics for such logics (e.g. Kripke's supervaluationism in the semantics of logic).

Logic and the philosophy of language are closely related. Philosophy of language has to do with the study of how our language engages and interacts with our thinking. Logic has an immediate impact on other areas of study. Studying logic and the relationship between logic and ordinary speech can help a person better structure his own arguments and critique the arguments of others. Many popular arguments are filled with errors because so many people are untrained in logic and unaware of how to formulate an argument correctly.[56][57]

Computational logic
Main articles: Computational logic and Logic in computer science

A simple toggling circuit is expressed using a logic gate and a synchronous register.
Logic cut to the heart of computer science as it emerged as a discipline: Alan Turing's work on the Entscheidungsproblem followed from Kurt Gödel's work on the incompleteness theorems. The notion of the general purpose computer that came from this work was of fundamental importance to the designers of the computer machinery in the 1940s.

In the 1950s and 1960s, researchers predicted that when human knowledge could be expressed using logic with mathematical notation, it would be possible to create a machine that mimics the problem-solving skills of a human being. This was more difficult than expected because of the complexity of human reasoning. In the summer of 1956, John McCarthy, Marvin Minsky, Claude Shannon and Nathan Rochester organized a conference on the subject of what they called "artificial intelligence" (a term coined by McCarthy for the occasion). Newell and Simon proudly presented the group with the Logic Theorist and were somewhat surprised when the program received a lukewarm reception.

In logic programming, a program consists of a set of axioms and rules. Logic programming systems such as Prolog compute the consequences of the axioms and rules in order to answer a query.

Today, logic is extensively applied in the field of artificial intelligence, and this field provide a rich source of problems in formal and informal logic. Argumentation theory is one good example of how logic is being applied to artificial intelligence. The ACM Computing Classification System in particular regards:

Section F.3 on "Logics and meanings of programs" and F.4 on "Mathematical logic and formal languages" as part of the theory of computer science: this work covers formal semantics of programming languages, as well as work of formal methods such as Hoare logic;
Boolean logic as fundamental to computer hardware: particularly, the system's section B.2 on "Arithmetic and logic structures", relating to operatives AND, NOT, and OR;
Many fundamental logical formalisms are essential to section I.2 on artificial intelligence, for example modal logic and default logic in Knowledge representation formalisms and methods, Horn clauses in logic programming, and description logic.
Furthermore, computers can be used as tools for logicians. For example, in symbolic logic and mathematical logic, proofs by humans can be computer-assisted. Using automated theorem proving, the machines can find and check proofs, as well as work with proofs too lengthy to write out by hand.

Non-classical logic
Main article: Non-classical logic
The logics discussed above are all "bivalent" or "two-valued"; that is, they are most naturally understood as dividing propositions into true and false propositions. Non-classical logics are those systems that reject various rules of Classical logic.

Hegel developed his own dialectic logic that extended Kant's transcendental logic but also brought it back to ground by assuring us that "neither in heaven nor in earth, neither in the world of mind nor of nature, is there anywhere such an abstract 'either–or' as the understanding maintains. Whatever exists is concrete, with difference and opposition in itself".[58]

In 1910, Nicolai A. Vasiliev extended the law of excluded middle and the law of contradiction and proposed the law of excluded fourth and logic tolerant to contradiction.[59] In the early 20th century Jan Łukasiewicz investigated the extension of the traditional true/false values to include a third value, "possible" (or an indeterminate, a hypothesis) so inventing ternary logic, the first multi-valued logic in the Western tradition.[60] A minor modification of the ternary logic was later introduced in a sibling ternary logic model proposed by Stephen Cole Kleene. Kleene's system differs from the Łukasiewicz's logic with respect to an outcome of the implication. The former assumes that the operator of implication between two hypotheses produces a hypothesis.

Logics such as fuzzy logic have since been devised with an infinite number of "degrees of truth", represented by a real number between 0 and 1.[61]

Intuitionistic logic was proposed by L.E.J. Brouwer as the correct logic for reasoning about mathematics, based upon his rejection of the law of the excluded middle as part of his intuitionism. Brouwer rejected formalization in mathematics, but his student Arend Heyting studied intuitionistic logic formally, as did Gerhard Gentzen. Intuitionistic logic is of great interest to computer scientists, as it is a constructive logic and sees many applications, such as extracting verified programs from proofs and influencing the design of programming languages through the formulae-as-types correspondence.

Modal logic is not truth conditional, and so it has often been proposed as a non-classical logic. However, modal logic is normally formalized with the principle of the excluded middle, and its relational semantics is bivalent, so this inclusion is disputable.

Controversies
"Is Logic Empirical?"
Further information: Is Logic Empirical?
What is the epistemological status of the laws of logic? What sort of argument is appropriate for criticizing purported principles of logic? In an influential paper entitled "Is Logic Empirical?"[62] Hilary Putnam, building on a suggestion of W. V. Quine, argued that in general the facts of propositional logic have a similar epistemological status as facts about the physical universe, for example as the laws of mechanics or of general relativity, and in particular that what physicists have learned about quantum mechanics provides a compelling case for abandoning certain familiar principles of classical logic: if we want to be realists about the physical phenomena described by quantum theory, then we should abandon the principle of distributivity, substituting for classical logic the quantum logic proposed by Garrett Birkhoff and John von Neumann.[63]

Another paper of the same name by Michael Dummett argues that Putnam's desire for realism mandates the law of distributivity.[64] Distributivity of logic is essential for the realist's understanding of how propositions are true of the world in just the same way as he has argued the principle of bivalence is. In this way, the question, "Is Logic Empirical?" can be seen to lead naturally into the fundamental controversy in metaphysics on realism versus anti-realism.

Implication: strict or material
Main article: Paradoxes of material implication
The notion of implication formalized in classical logic does not comfortably translate into natural language by means of "if ... then ...", due to a number of problems called the paradoxes of material implication.

The first class of paradoxes involves counterfactuals, such as If the moon is made of green cheese, then 2+2=5, which are puzzling because natural language does not support the principle of explosion. Eliminating this class of paradoxes was the reason for C. I. Lewis's formulation of strict implication, which eventually led to more radically revisionist logics such as relevance logic.

The second class of paradoxes involves redundant premises, falsely suggesting that we know the succedent because of the antecedent: thus "if that man gets elected, granny will die" is materially true since granny is mortal, regardless of the man's election prospects. Such sentences violate the Gricean maxim of relevance, and can be modelled by logics that reject the principle of monotonicity of entailment, such as relevance logic.

Tolerating the impossible
Main article: Paraconsistent logic
Georg Wilhelm Friedrich Hegel was deeply critical of any simplified notion of the law of non-contradiction. It was based on Gottfried Wilhelm Leibniz's idea that this law of logic also requires a sufficient ground to specify from what point of view (or time) one says that something cannot contradict itself. A building, for example, both moves and does not move; the ground for the first is our solar system and for the second the earth. In Hegelian dialectic, the law of non-contradiction, of identity, itself relies upon difference and so is not independently assertable.

Closely related to questions arising from the paradoxes of implication comes the suggestion that logic ought to tolerate inconsistency. Relevance logic and paraconsistent logic are the most important approaches here, though the concerns are different: a key consequence of classical logic and some of its rivals, such as intuitionistic logic, is that they respect the principle of explosion, which means that the logic collapses if it is capable of deriving a contradiction. Graham Priest, the main proponent of dialetheism, has argued for paraconsistency on the grounds that there are in fact, true contradictions.[65][clarification needed]

Rejection of logical truth
The philosophical vein of various kinds of skepticism contains many kinds of doubt and rejection of the various bases on which logic rests, such as the idea of logical form, correct inference, or meaning, typically leading to the conclusion that there are no logical truths. This is in contrast with the usual views in philosophical skepticism, where logic directs skeptical enquiry to doubt received wisdoms, as in the work of Sextus Empiricus.

Friedrich Nietzsche provides a strong example of the rejection of the usual basis of logic: his radical rejection of idealization led him to reject truth as a "... mobile army of metaphors, metonyms, and anthropomorphisms—in short ... metaphors which are worn out and without sensuous power; coins which have lost their pictures and now matter only as metal, no longer as coins".[66] His rejection of truth did not lead him to reject the idea of either inference or logic completely but rather suggested that "logic [came] into existence in man's head [out] of illogic, whose realm originally must have been immense. Innumerable beings who made inferences in a way different from ours perished".[67] Thus there is the idea that logical inference has a use as a tool for human survival, but that its existence does not support the existence of truth, nor does it have a reality beyond the instrumental: "Logic, too, also rests on assumptions that do not correspond to anything in the real world".[68]

This position held by Nietzsche however, has come under extreme scrutiny for several reasons. Some philosophers, such as Jürgen Habermas, claim his position is self-refuting—and accuse Nietzsche of not even having a coherent perspective, let alone a theory of knowledge.[69] Georg Lukács, in his book The Destruction of Reason, asserts that, "Were we to study Nietzsche's statements in this area from a logico-philosophical angle, we would be confronted by a dizzy chaos of the most lurid assertions, arbitrary and violently incompatible."[70] Bertrand Russell described Nietzsche's irrational claims with "He is fond of expressing himself paradoxically and with a view to shocking conventional readers" in his book A History of Western Philosophy.[71